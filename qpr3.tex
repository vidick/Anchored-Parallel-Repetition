
\section{Parallel repetition of anchored games}
\label{sec:analysis}

This section is devoted to the analysis of the entangled value of repeated two-player anchored games. The main theorem we prove is the following:

%While we expect the arguments to carry over to the multiplayer case without any additional difficulty we leave this for future work and focus on two-player entangled games. We use $\X$ and $\Y$ (resp. $\A$ and $\B$) to denote the two players' respective question (resp. answer) sets in $G$, and following convention we name the first player Alice and the second Bob. Our main result is the following. 

\begin{theorem}[Main Theorem]\label{thm:anchorpr_quantum}
	There exists a universal constant $c > 0$ such that the following holds. Let $0<\alpha\leq 1$ and $G$ an $\alpha$-anchored game. Then for all $0 < \eps \leq 1$,
for all integers $n$, and for all $p$ satisfying
\begin{equation}
\label{eq:p}
p \geq \frac{4}{\eps} \, \exp \Big ( - \frac{c \cdot \alpha^{48} \cdot \eps^{16} \cdot n}{s} \Big)\;,
%p \geq \exp \left ( - \Omega \left( \frac{\alpha^{48} \cdot \eps^{16} \cdot n}{s}  \right) \right)
\end{equation}
where  $s =\lceil \log |\A \times \B|\rceil$\tnote{added ceilings}, 
\[
	\E(G^n,p) \geq \E(G,1 - \eps)\;.
\]
\end{theorem}

We give an easy corollary that provides exponential-decay bounds on the entangled value of repeated anchored games.

\begin{corollary}
Let $G$ be an $\alpha$-anchored game satisfying $\eval(G) < 1-\eps$. Then
	$$ \eval(G^n)\leq \frac{4}{\eps} \, \exp \Big ( - \frac{c \cdot \alpha^{48} \cdot \eps^{16} \cdot n}{s} \Big)~.$$
\end{corollary}
\begin{proof}
	Suppose that $\eval(G^n)$ is larger than 
	\[p = \frac{4}{\eps} \, \exp \Big ( - \frac{c \cdot \alpha^{48} \cdot \eps^{16} \cdot n}{s} \Big)\;.\]
	Then $\E(G^n,p) < +\infty$, which by Theorem~\ref{thm:anchorpr_quantum} and the choice of $p$ implies that $\E(G,1 - \eps) < \infty$. This contradicts the assumption that $\eval(G) < 1 - \eps$.
\end{proof}

%Thus the anchoring operation provides a general gap amplification transformation for the entangled value of any multiplayer game.

The remainder of this section is devoted to the proof of \Cref{thm:anchorpr_quantum}. Fix an $\alpha$-anchored two-player game $G = (\X \times \Y,\A \times \B,\mu,V)$. Fix an $\eps > 0$, integer $n\geq 1$ and $p$ satisfying \Cref{eq:p}. First suppose that $\E(G^n,p) = +\infty$; then $\E(G^n,p) \geq \E(G,1 - \eps)$ trivially holds. The other case is that $\E(G^n,p) = d$ for some finite $d$. Let $\strategy^n = (\ket{\psi},A,B)$ be a $d$-dimensional strategy for $G^n$ with value $\eval(G^n,\strategy^n) = p$. By conjugating the second player's measurement operators by a unitary $U$, and exchanging $\ket{\psi}$ with $\Id \otimes U \ket{\psi}$, we may assume without loss of generality that $\ket{\psi}$ is  invariant under permutation, i.e.\ can be written as 
\begin{equation}\label{eq:psi-sym}
 \ket{\psi} \,=\, \sum_j \sqrt{\lambda_j} \ket{v_j} \ket{v_j}\,\in \, \C_{E_A}^d \otimes \C_{E_B}^d
\end{equation}
for an orthonormal basis $\{ \ket{v_j} \}_j$. Our goal is to show the existence of a $d$-dimensional strategy $\strategy$ for $G$ that succeeds with probability at least $1 - \eps$. 


%Fix a strategy for $G^n$, consisting of a shared entangled state $\ket{\psi} \in \C_{E_A}^d \otimes \C_{E_B}^d$ and POVMs $\{A_{\bx}^{\ba} \}$ and $\{B_{\by}^{\bb}\}$ for Alice and Bob respectively. Without loss of generality we assume that $\ket{\psi}$ is invariant under permutation of the two registers, i.e. there exist basis vectors $\{ \ket{v_j} \}_j$ such that $\ket{\psi} = \sum_j \sqrt{\lambda_j} \ket{v_j} \ket{v_j}$. \footnote{This is because we can always conjugate Bob's operators with the inverse of a unitary map $U$ such that $\Id \otimes U \ket{\psi}$ is permutation invariant.} 



\subsection{Dependency-breaking variables, states, and measurements}
\label{sec:quantum_setup}

We introduce the random variables, entangled states and operators used in the proof of Theorem~\ref{thm:anchorpr_quantum}. The section is divided into three parts: first we define all the random variables and their joint distribution $\P$; in particular we define the dependency-breaking variable $\bOmega$. Then we state useful lemmas about conditioned distributions. Finally we describe the states and operators used in the proof.

%We use capital letters $\bX, \bY, \bA, \bB, X, Y, A, B$ to denote random variables, and corresponding lower-case letters $\bx,\by,\ba,\bb, x, y, a,b$ to denote instantiations of the random variables. Throughout this paper we will write consider marginal and conditional distributions of $\P$ with respect to different random variables. For example, we write $\P_{\bA \bB | \bX = \bx, \bY = \by}(\ba,\bb)$ to denote the distribution of random variables $\bA \bB$, \emph{conditioned} on $\bX = \bx, \bY = \by$. For brevity we often write this as $\P_{\bA \bB | \bx, \by}$ when it is clear from context which random variables we are conditioning on.


\subsubsection{The probability measure $\P$} 
\label{sec:p_setup}
Let $\mu_X$ and $\mu_Y$ denote the marginals of the game distribution $\mu_{XY}$ on the first and second coordinates, respectively. 
We first define a ``single copy'' distribution $\hat{P}$ as the law of random variables $(D,M,X,Y)$ defined as follows. Each random variable may depend on previously defined ones. We start with $D$, which is distributed uniformly over $\{A,B\}$. Fix a ``noise'' parameter $\eta = \alpha/2$. Let $M$ have the following distribution over $\X\times \Y$: for all $x \in \X$ and $y \in \Y$,
\[
	\P_{M | D = A}(x) = \left\{
	\begin{array}{ll}
		\mu_X(x)/(1 - \eta)  & \mbox{if $x \neq \dummy$} \\
		(\alpha - \eta)/(1 - \eta) & \mbox{if $x = \dummy$ }
	\end{array}
\right. \quad \text{and} \quad
	\P_{M | D = B}(y) = \left\{
	\begin{array}{ll}
		\mu_Y(y)/(1 - \eta)  & \mbox{if $y \neq \dummy$} \\
		(\alpha - \eta)/(1 - \eta) & \mbox{if $y = \dummy$ }
	\end{array}
\right.
\]
In other words, conditioned on $D=A$ (resp. $D = B$), the variable $M$ takes on a value in $\X$ (resp. $\Y$) from a rescaled version of the distribution $\mu_X$ (resp. $\mu_Y$) where less weight is given to the dummy question $\dummy$. Finally, define the random variables $(X,Y)$ as follows. 
\begin{itemize}
	\item If $D = A$ then $X$ is chosen to be an ``$\eta$-noisy'' copy of $M$. Precisely, $X = M$ with probability $1 - \eta$ and $X = \dummy$ with probability $\eta$. Define $Y$ to equal $y$ with probability $\mu_{Y|X}(y | m)$, where $m$ is the value of $M$. 
	\item If $D = B$ then $Y$ is chosen to be an ``$\eta$-noisy'' copy of $M$ and $X$ equals $x$ with probability $\mu_{X|Y}(x | m)$, where $m$ is the value of $M$. 
\end{itemize}
This specifies the distribution $\hat{P}$. 


\begin{claim}
\label{clm:dependency-breaking-1}
	Conditioned on $(D,M)$ the random variables $X$ and $Y$ are independent. 
\end{claim}
\begin{proof}
It follows directly from the construction that for every $(d,m)$ and $(x,y)$, 
\[
		\hat{\P}_{XY | D=d,M=m}(x,y) = \hat{\P}_{X | D=d,M=m}(x) \cdot \hat{\P}_{Y | D=d,M=m}(y)\;.\]
\end{proof}


\begin{claim}
\label{clm:dependency-breaking-2}
 $\hat{\P}_{XY|D = A}(x,y) = \hat{\P}_{XY|D=B}(x,y) = \mu_{XY}(x,y)$ for all $x \in \X,y \in \Y$. In particular, the marginal distribution $\hat{\P}_{XY}$ is the game distribution $\mu$. 
\end{claim}

\begin{proof}
	Fix $D = A$, and suppose that $x = \dummy$. Using the conditional independence stated in Claim~\ref{clm:dependency-breaking-1} we can write 
	\begin{align*}
		\hat{\P}_{XY | D = A}(\dummy,y) &= \sum_{x' \in \X} \hat{\P}_{M | D = A}(x') \cdot \hat{\P}_{X | M=x',D=A}(\dummy) \cdot  \hat{\P}_{Y | M = x', D = A}(y)  \\
		&= \hat{\P}_{M | D = A}(\dummy) \cdot \hat{\P}_{X | M = \dummy, D = A}(\dummy) \cdot  \hat{\P}_{Y | M = \dummy, D = A}(y) \\
		& \qquad \qquad + \sum_{x' \in \X \setminus \{\dummy\}} \hat{\P}_{M | D = A}(x') \cdot \hat{\P}_{X | M = x', D = A}(\dummy) \cdot  \hat{\P}_{Y | M = x, D = A}(y) \\
		&= \frac{\alpha - \eta}{1 - \eta} \cdot \mu_Y(y) + \frac{\eta}{1 - \eta} \sum_{x' \in \X \setminus \{\dummy\}} \mu_X(x') \cdot \mu_{Y|X}(y|x') \\
		&= \frac{\alpha - \eta}{1 - \eta} \cdot \mu_Y(y) + \frac{\eta}{1 - \eta} \Big ( \mu_Y(y) - \mu_{XY}(\dummy,y) \Big ) \\
		&= \frac{\alpha - \eta}{1 - \eta} \cdot \mu_Y(y) + \frac{\eta}{1 - \eta} \Big ( \mu_Y(y) - \mu_{Y}(y) \cdot \alpha \Big ) \\
		&= \alpha \cdot \mu_Y(y) \\
		&= \mu_{XY}(\dummy,y)~.
	\end{align*}
	Now suppose that $x \neq \dummy$. The random variable $X$ can only take this value if $M = x$, so we have
	\begin{align*}
		\hat{\P}_{XY | D = A}(x,y) &= \hat{\P}_{M | D = A}(x) \cdot \hat{\P}_{X | M = x, D = A}(x) \cdot  \hat{\P}_{Y | M=x, D=A}(y) \\
		&= \frac{1}{1 - \eta} \cdot \mu_X(x) \cdot (1 - \eta) \cdot \mu_{Y|X}(y | x) \\
		&= \mu_{XY}(x,y)~.
	\end{align*}
	A symmetric argument holds for the case $D = B$. This shows the claim.
\end{proof}



We define a distribution $\P$ as follows. Let $\bD = (\bD_1,\ldots,\bD_n)$, $\bM = (\bM_1,\ldots,\bM_n)$, $\bX = (\bX_1,\ldots,\bX_n)$, and $\bY = (\bY_1,\ldots,\bY_n)$ be vectors of random variables, and define
\[
	\P_{\bD \bM \bX \bY} = \prod_{i=1}^n \hat{\P}_{\bD_i \bM_i \bX_i \bY_i}~.
\]
Finally, define random variables $\bA = (\bA_i)_{i \in [n]}$ and $\bB = (\bB_i)_{i \in [n]}$ as follows. Conditioned on $\bX$ and $\bY$, the random variables $\bA,\bB$ are independent of $\bD$ and $\bM$. For all realizations $\bx, \by$ of $\bX$ and $\bY$, define
\[
	\P_{\bA \bB | \bX = \bx, \bY = \by}(\ba,\bb) = \bra{\psi} A_\bx(\ba) \otimes B_\by(\bb) \ket{\psi}
\]
for all $\ba = (\ba_1,\ldots,\ba_n) \in \A^n$ and $\bb = (\bb_1,\ldots,\bb_n) \in \B^n$. $\bA$ and $\bB$ are jointly distributed as answers produced by players using strategy $\strategy^n$ when their question pair for the $i$-th game is $(\bx_i,\by_i)$. 
Since conditioned on $\bX, \bY$, $\bA$ and $\bB$ are independent of $\bD,\bM$ we have that the marginal distribution of $\bX \bY \bA \bB$ is
	\[
		\P(\bx,\by,\ba,\bb) = \Big ( \prod_{i=1}^n \mu(\bx_i,\by_i) \Big) \cdot \bra{\psi} A_\bx(\ba) \otimes B_\by(\bb) \ket{\psi}\;,
	\]
	which matches the joint distribution of questions and answers in the repeated game $G^n$ when the players use the strategy $\strategy^n$.

%
%
%\begin{claim}
%\label{clm:dependency-breaking-3}
%	The marginal distribution of the random variables $\bX \bY \bA \bB$ is
%	\[
%		\P(\bx,\by,\ba,\bb) = \Big ( \prod_{i=1}^n \mu(\bx_i,\by_i) \Big) \cdot \bra{\psi} A^{\ba}_\bx \otimes B^{\bb}_\by \ket{\psi}.
%	\]
%\end{claim}
%\begin{proof}
%	Since conditioned on $\bX, \bY$, the answer variables $\bA$ and $\bB$ are independent of $\bD,\bM$, it suffices to show that the marginal distribution of $\bX$ and $\bY$ is equal to $\prod_{i=1}^n \mu(\bx_i,\by_i)$ -- in other words, the question distribution of the repeated game $G^n$.
%	
%\end{proof}


\subsubsection{Dependency-breaking variables} 
\label{sec:dep-var}

 
We introduce \emph{dependency-breaking variables}. These are crucial for controlling the correlations that arise in the analysis when conditioning the  distribution $\P$ on different events. 

Let $C \subseteq [n]$ be a set of coordinates for the repeated game $G^n$. 
Let $(\bX_C,\bY_C)$ and $(\bA_C,\bB_C)$ denote the players' questions and answers respectively associated with the coordinates indexed by $C$. For $i \in [n]$ let $W_i$ denote the indicator variable for the event that the players win round $i$; i.e. $V(\bX_i,\bY_i,\bA_i,\bB_i) = 1$. Let $W_C = \prod_{i \in C} W_i$. 

For all $i \in [n] \setminus C$ define the $i$-th dependency-breaking variable $\bOmega_i$ as $\bOmega_i = (\bD_i,\bM_i)$. When $\eta = 0$ this definition coincides with the one used by Holenstein in his proof of the classical parallel repetition theorem~\cite{Hol09}; in that case, the variable $\bM_i$ is coupled to either $\bX_i$ or $\bY_i$ exactly. Here we set $\eta$ to be a nonzero value that is related to $\alpha$, the anchoring probability. This ``noisy coupling'' between $\bOmega_i$ and the inputs $(\bX_i,\bY_i)$ is important for our analysis. 


%fixes one of $\bX_i$ or $\bY_i$. Thus, conditioned on $\bOmega_i$, $\bX_i$ and $\bY_i$ are independent of each other. This is the same dependency-breaking variable used in Holenstein's proof of parallel repetition. More precisely, 


Define $\bOmega = (\bOmega_i)_{i \in [n] \setminus C} \cup (\bX_C,\bY_C)$. We denote realizations of the random variable $\bOmega$ using $\bomega = (\bomega_i)_{i \in [n] \setminus C} \cup (\bx_C,\by_C)$. For $i \in [n] \setminus C$ we write $\bOmega_\mi$ to denote $(\bOmega_j)_{j \in [n] \setminus (C \cup \{i\})} \cup (\bX_C,\bY_C)$ and $\bomega_\mi$ to denote instantiations of $\bOmega_\mi$. We furthermore define variables $\bR = (\bOmega,\bA_C,\bB_C)$ and $\bR_\mi = (\bOmega_\mi,\bA_C,\bB_C)$ with realizations $\br = (\bomega,\ba_C,\bb_C)$ and $\br_\mi = (\bomega_\mi,\ba_C,\bb_C)$ respectively.

\tnote{I think here we should insert a claim with easy facts that are used in various places; it would help the reader not scratch their head every time a random claim such as $\P_{\bA_C \bB_C | \bX \bY} = \P_{\bA_C \bB_C | \bOmega \bX \bY}$ is made}



%This is the same dependency-breaking variable used in Holenstein's proof of parallel repetition. The reason it is called ``dependency-breaking'' is because, conditioned on $\bR$, the questions $\bX$ and $\bY$ are independent:
%
%\begin{proposition}
%\label{prop:cond-independence}
%For all realizations $\br$ of the random variable $\bR$, the following holds:
%\[
%	\P_{\bX \bY | \bR = \br} = \P_{\bX | \bR = \br} \cdot \P_{\bY | \bR = \br}\;.
%\]
%\end{proposition}
%\begin{proof}
%For all $\bx \in \X^n,\by \in \Y^n$, we have $\P_{\bX \bY | \bR = \br}(\bx,\by) &= \P_{\bX | \bR = \br}(\bx) \cdot \P_{\bY | \bR = \br, \bX = \bx}(\by)$.
%\begin{align}
%	\P_{\bY | \bR = \br, \bX = \bx}(\by) &= \frac{\P_{\bX \bY \bR}(\bx,\by,\br)}{\P_{\bX = }$
%\end{align}
%
%\end{proof}
% 
\subsubsection{Individual coordinates are relatively unaffected by conditioning} 

Let $C\subseteq [n]$ be a fixed subset of coordinates such that $\P(W_C) > 0$, and let $m=n-C$. For convenience, up to relabeling we generally assume that $C$ contains the last $n-m$ coordinates $C=\{m+1,\ldots,n\}$. Let
\begin{equation}
\label{eq:delta}
	\delta = \frac{1}{m} \left ( \log \frac{1}{\P(W_C)} + |C| \log |\A| |\B| \right)\;.
\end{equation}

\tnote{The next paragraph is very imprecise and false. Could we remove it?} \hnote{I wanted to add some intuitive explanation of what the next Lemma is trying to say. what part is false? The $2^{-m}$?}\tnote{Yes. The main problem though is that I can't understand many words. What does it mean to be ``insensitive''? To ``augment'' a distribution with an ``index''? An ``instantiation'' of a variable? Generally I find such ``intuitive'' explanations good for you maybe, but here I find it much easier to read the statement of the lemma and think what it means rather than parse the vague explanation}
The following lemma shows that if the event $W_C$ occurs with probability significantly larger than $2^{-m}$ then conditioning on $W_C$ does not significantly affect the distribution of the inputs $(\bX_i, \bY_i)$ in a randomly chosen coordinate $i \in [n] \setminus C$, and furthermore the distribution of the the dependency-breaking variable $\bR_\mi$ is insensitive to the questions $(\bX_i,\bY_i)$. In the Lemma, we augment the probability distribution $\P$ with an index $I$ that is chosen from $[n] \setminus C$ uniformly at random, and we let $i$ denote the instantiation of the variable $I$.

\begin{lemma}
\label{lem:classical_skew}
The following inequalities hold. \tnote{The notation with $I$ and $i$ is bad. Could we write $\frac{1}{m}\sum_i$ as in the proof? Also, the proof seems to give bounds on a random $i$ from $1$ to $m$. Is this some kind of bijection with $[n]\backslash C$?} \hnote{Re: $I$ vs $i$; this is to be consistent with how we treat all the other random variables $X,Y, \bOmega$ versus their instantiations $x,y,\bomega$, etc. We could switch to $\frac{1}{m} \sum_{i \in [n] \setminus C}$ everywhere; the rest of the proofs use $\Ex_I$ and $\P_I$ to save space and be consistent. Doesn't matter to me as long as we're consistent.}\tnote{Ok so here I put the sums. Earlier I also wrote the convention that $[n]\backslash C = \{1,\ldots,m\}$}
\begin{enumerate}
	\item $\frac{1}{m}\sum_{i=1}^m \| \P_{\bOmega_i \bX_i \bY_i | W_C} -  \P_{\bOmega_i \bX_i \bY_i} \| \leq \sqrt{\delta}$.
	\item $\frac{1}{m}\sum_{i=1}^m\|  \P_{\bR \bX_i \bY_i | W_C} -  \P_{\bR |W_C} \P_{\bX_i \bY_i | \bOmega_i } \| \leq \sqrt{\delta}$.
	\item $\frac{1}{m}\sum_{i=1}^m\| \P_{\bOmega_i |W_C} \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} -\P_{\bOmega_i | W_C} \P_{\bR_\mi |\bOmega_i  W_C}  \Big \| \leq O(\sqrt{\delta}/\alpha^2)$.
	\item $\frac{1}{m}\sum_{i=1}^m\|  \P_{\bX_i \bY_i}  \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} -  \P_{\bX_i \bY_i}  \P_{\bR_\mi | \bX_i, \bY_i, W_C} \| \leq O(\sqrt{\delta}/\alpha^2)$.
%	\item $\left \| \P_I \P_{\bX_i \bY_i} \P_{\bR_\mi | \bX_i \bY_i W_C} - \P_I \P_{\bX_i \bY_i \bR_\mi | W_C} \right \| \leq \sqrt{\delta}$
\end{enumerate}
\end{lemma}

The proof of Lemma~\ref{lem:classical_skew} makes use of Lemma 4.1 and Corollary 4.3 from~\cite{Hol09}, which we restate for convenience. 

\begin{lemma}[Lemma 4.1 of~\cite{Hol09}]
\label{lem:hol}
Let $\bU = (\bU_1,\ldots,\bU_m)$ be a vector of random variables and let $\P_{\bU} = \P_{\bU_1} \cdots \P_{\bU_m}$. Let $W$ denote an event. Then
\[
	\sum_{i=1}^m \, \Big \| \P_{\bU_i} - \P_{\bU_i | W} \Big \|^2 \leq \log \frac{1}{\P(W)}~.
\]
\end{lemma}

\begin{corollary}[Corollary 4.3 of~\cite{Hol09}]
\label{cor:hol}
Let $T$, $V$ be random variables and let $\bU = (\bU_1,\ldots,\bU_m)$ be a vector of random variables. Let $\P_{T\bU V} = \P_T \cdot \P_{\bU_1 | T} \cdots \P_{\bU_m} \cdot \P_{V|T \bU}$. Let $W$ be an event. Then
\[
	\frac{1}{m} \sum_{i=1}^m \Big \| \P_{T \bU_i V|W} - \P_{TV|W} \P_{\bU_i | T} \Big \| \leq \sqrt{ \frac{1}{m} \left ( \log(|\mathcal{V}^*|) + \log \frac{1}{\P(W)}  \right)}
\]
where $\mathcal{V}^* = \{ v : \P_{V|W}(v) > 0 \}$. 
\end{corollary}

\begin{proof}[Proof of \Cref{lem:classical_skew}]
\begin{enumerate}
\item For $i\in [n]\backslash C$ let $\bU_i$ denote the tuple $(\bOmega_i,\bX_i,\bY_i)$, and let $W$ denote the event $W_C$. Note that all of the $\bU_i$ are independent of each other by construction, and thus applying \Cref{lem:hol} we get
\[
\sum_{i \in [n] \setminus C} \, \Big \| \P_{\bOmega_i \bX_i \bY_i} - \P_{\bOmega_i \bX_i \bY_i | W_C} \Big \|^2 \leq \log \frac{1}{\P(W_C)}~.
\]
Dividing by $m$ on both sides and using Jensen's inequality, we get
\[
\frac{1}{m} \sum_{i \in [n] \setminus C} \, \Big \| \P_{\bOmega_i \bX_i \bY_i} - \P_{\bOmega_i \bX_i \bY_i | W_C} \Big \| \leq \sqrt{ \frac{1}{m} \sum_{i \in [n] \setminus C} \, \Big \| \P_{\bOmega_i \bX_i \bY_i} - \P_{\bOmega_i \bX_i \bY_i | W_C} \Big \|^2} \leq \sqrt{ \frac{1}{m} \log \frac{1}{\P(W_C)}}
\]
which by definition of $\delta$ in~\eqref{eq:delta} is at most $\sqrt{\delta}$. %We conclude by observe that $\| \P_I \P_{\bOmega_i \bX_i \bY_i} - \P_I \P_{\bOmega_i \bX_i \bY_i | W_C} \| = \frac{1}{m} \sum_{i \in [n] \setminus C} \, \Big \| \P_{\bOmega_i \bX_i \bY_i} - \P_{\bOmega_i \bX_i \bY_i | W_C} \Big \|$.

\item Let $T$ denote the random variable $\bOmega$, let $\bU_i$ denote the pair $(\bX_i,\bY_i)$, and let $V$ denote the pair $(\bA_C,\bB_C)$. Since $\bU_1,\ldots,\bU_m$ are independent, even conditioned on $\bOmega$ which has a product distribution, we can apply 
\Cref{cor:hol} and get
\begin{equation}
\label{eq:classical-skew-1}
	\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bR \bX_i \bY_i|W_C} - \P_{\bR|W_C} \P_{\bX_i \bY_i | \bOmega} \Big \| \leq \sqrt{ \frac{1}{m} \left ( \log \Big ( (|\A| \cdot |\B|)^{|C|} \Big) + \log \frac{1}{\P(W_C)}  \right)} = \sqrt{\delta}\;,
\end{equation}
where we used the fact that the support of $V$ has size at most $(|\A| \cdot |\B|)^{|C|}$. We then use that $\P_{\bX_i \bY_i | \bOmega} = \P_{\bX_i \bY_i | \bOmega_i}$ to obtain the second item.

\item We start by rewriting~\eqref{eq:classical-skew-1} using Bayes' rule as
\begin{equation}
\label{eq:classical-skew-2}
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C} \P_{\bX_i \bY_i | \bOmega_i W_C} \P_{\bR_\mi | \bOmega_i \bX_i \bY_i W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi |\bOmega_i  W_C}  \Big \| \leq \sqrt{\delta}\;.
\end{equation}
We note that $\P_{\bR_\mi | \bOmega_i \bX_i \bY_i W_C} = \P_{\bR_\mi |\bX_i \bY_i W_C}$. This follows from Bayes' rule and the observation that 
\[\P(W_C | \bR_\mi \bOmega_i \bX_i \bY_i) = \P(W_C | \bR_\mi \bX_i \bY_i)\qquad\text{and}\qquad \P(W_C | \bOmega_i \bX_i \bY_i) = \P(W_C | \bX_i \bY_i)\;,\] which in turn follow from the fact that the distribution of $(\bA_C,\bB_C)$, conditioned on $(\bX,\bY)$, does not depend on $\bOmega$. \tnote{Sorry I'm still missing it. I seem to need that $\P(\bR_\mi |\bOmega_i \bX_i \bY_i) = \P(\bR_\mi | \bX_i \bY_i)$. I think this is true but it should be said, no?}

Item 1 of the Lemma, combined with \Cref{lem:trivial}\tnote{What does the lemma have to do with it? Are you not just marginalizing XY? So this is data processing?}, implies that 
\begin{equation}\label{eq:classical-skew-2bb}
\frac{1}{m} \sum_{i \in [n] \setminus C} \| \P_{\bOmega_i | W_C} - \P_{\bOmega_i}  \| \leq \sqrt{\delta}\;.
\end{equation}
Using \Cref{lem:trivial} again, we get that $\frac{1}{m} \sum_{i \in [n] \setminus C} \| \P_{\bOmega_i } \P_{\bX_i \bY_i | \bOmega_i} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \| \leq \sqrt{\delta}$ and thus combined with Item 1 and the triangle inequality we have $\frac{1}{m} \sum_{i \in [n] \setminus C} \| \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \| \leq 2\sqrt{\delta}$. 
%Next we use Item 1 of the Lemma twice\tnote{No, it doesn't apply. I think you're using item 1 once, and Lemma~\ref{lem:hol} for the $\Omega_i$. Note that you use this again later, so it could be stated separately} 
% to get $\frac{1}{m} \sum_{i \in [n] \setminus C} \| \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \| \leq 2\sqrt{\delta}$. 
\Cref{lem:trivial} then implies that~\eqref{eq:classical-skew-2} is within at most $2\sqrt{\delta}$ of
\begin{equation}
\label{eq:classical-skew-2b}
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi | \bX_i \bY_i W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi |\bOmega_i  W_C}  \Big \|
\end{equation}
Notice that conditioned on $\bOmega_i$, by construction the variables $(\bX_i,\bY_i)$ take on the value $(\dummy,\dummy)$ with probability at least $\eta^2 = \Omega(\alpha^2)$. Thus conditioning both sides of the difference on $(\bX_i,\bY_i) = (\dummy,\dummy)$ we can get that 
\begin{equation}
\label{eq:classical-skew-3}
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C} \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} - \P_{\bOmega_i | W_C} \P_{\bR_\mi |\bOmega_i  W_C}  \Big \| \leq O \Big( \frac{\sqrt{\delta}}{\alpha^2} \Big),
\end{equation}
which establishes the third item.

\item We insert a fresh copy of $\P_{\bX_i \bY_i | \bOmega_i}$ on both sides of the difference in each term of\eqref{eq:classical-skew-2b} and~\eqref{eq:classical-skew-3} to get, using Lemma~\ref{lem:trivial},
\begin{gather*}
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C}\P_{\bX_i \bY_i | \bOmega_i}  \P_{\bR_\mi | \bX_i, \bY_i, W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi |\bOmega_i  W_C}  \Big \| \leq O(\sqrt{\delta})~, \text{ and} \\
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C}\P_{\bX_i \bY_i | \bOmega_i}  \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi |\bOmega_i  W_C}  \Big \| \leq O \Big( \frac{\sqrt{\delta}}{\alpha^2} \Big)~.
\end{gather*}
Using the triangle inequality with the two preceding inequalities we get
\[
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C}\P_{\bX_i \bY_i | \bOmega_i}  \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} - \P_{\bOmega_i | W_C} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi | \bX_i \bY_i  W_C}  \Big \| \leq O \Big( \frac{\sqrt{\delta}}{\alpha^2} \Big)~.
\]
Using~\eqref{eq:classical-skew-2bb}\tnote{Again, I don't think this follows from the first item} \hnote{this was argued previously}\tnote{yes but come on, do we want the proof to be readable or not? It involves 40 inequalities; how much does it cost to actually introduce a reference? I did this} combined with \Cref{lem:trivial} and the triangle inequality we get
\[
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bOmega_i |W_C}\P_{\bX_i \bY_i | \bOmega_i}  \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} - \P_{\bOmega_i} \P_{\bX_i \bY_i | \bOmega_i} \P_{\bR_\mi | \bX_i \bY_i  W_C}  \Big \| \leq O \Big( \frac{\sqrt{\delta}}{\alpha^2} \Big)~.
\]
Marginalizing over the random variable $\bOmega_i$, we get
\[
\frac{1}{m} \sum_{i \in [n] \setminus C} \Big \| \P_{\bX_i \bY_i}  \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy, W_C} - \P_{\bX_i \bY_i} \P_{\bR_\mi | \bX_i \bY_i  W_C}  \Big \| \leq O \Big( \frac{\sqrt{\delta}}{\alpha^2} \Big)~,
\]
which concludes the proof of Item 4.
\end{enumerate}
%Item 4 follows from Item 1: since $\frac{1}{m} \sum_{i \in [n] \setminus C} \| \P_{\bX_i \bY_i | W_C} - \P_{\bX_i \bY_i} \| \leq \sqrt{\delta}$, by the triangle inequality we have 
%\[
%	\frac{1}{m} \sum_{i \in [n] \setminus C} \|\P_{\bX_i \bY_i} \P_{\bR_\mi | \bX_i \bY_i W_C} - \P_{\bX_i \bY_i \bR_\mi | W_C}  \| \leq \sqrt{\delta}~.
%\]
\end{proof}

\subsubsection{Quantum states and operators}
\label{sec:states-operators} 

From the operators and state specified in the strategy $\strategy^n$ we define new operators and states that will be used in the analysis.

\paragraph{Operators.} For all $\bx \in \X^n, \by \in \Y^n$ let $\{A_\bx(\ba)\}_{\ba \in \A^n}$ and $\{B_\by(\bb)\}_{\bb \in \B^n}$ denote Alice and Bob's POVMs in the strategy $\strategy^n$ when receiving questions $\bx$ and $\by$, respectively. Define the following operators for all $\ba_C \in \A^C, \bb_C \in \B^C, \bx \in \X^n, \by \in \Y^n$:
\begin{equation}
\label{eq:states-operators-1}
	A_{\bx}(\ba_C) = \sum_{\ba | \ba_C} A_{\bx}(\ba) \qquad\text{and} \qquad B_{\by}(\bb_C) = \sum_{\bb | \bb_C} B_{\by}(\bb)
\end{equation}
where $\ba | \ba_C$ (resp. $\bb | \bb_C$) indicates summing over all tuples $\ba$ consistent with $\ba_C$ (resp. $\bb$ consistent with $\bb_C$). Note that for all $\bx$ (resp. $\by$), the set $\{ A_\bx(\ba_C) \}$ (resp. $\{ B_\by(\bb_C) \}$) denotes a POVM with outcomes in the set $\A^C$ (resp. $\B^C$).

For all $i \in [m]$, $\bomega_\mi$, $x \in \X$, and $y \in \Y$ define
\begin{equation}
\label{eq:states-operators-2}
 A_{\bomega_\mi, x}(\ba_C) = \Ex_{\bX | \bomega_\mi, x} A_{\bx}(\ba_C) \qquad \text{and} \qquad  B_{\bomega_\mi, y}(\bb_C) = \Ex_{\bY | \bomega_\mi, y} B_{\by}(\bb_C)
\end{equation}
where $\Ex_{\bX | \bomega_\mi, x}$ is shorthand for $\Ex_{\bX | \bOmega_\mi = \bomega_\mi, \bX_i = x}$ (and similarly for $\Ex_{\bY | \bomega_\mi, y}$).
Let $\X_{/\dummy} = \{ \dummyx,\; x\in \X\}$ be a disjoint copy of $\X$. Here, for each $x\in \X$, ``$\dummyx$'' is a new symbol that is used to distinguish elements in $\X$ from elements in $\X_{/\dummy}$. 
For all $\dummyx \in \X_{/\perp}$ define
\begin{equation}
\label{eq:states-operators-3}
	A_{\bomega_\mi,\dummyx}(\ba_C) = \eta \, A_{\bomega_\mi,\dummy}(\ba_C) + (1 - \eta) \, A_{\bomega_\mi,x}(\ba_C)\;.
\end{equation}
Using that all operators are positive semidefinite we observe for later use that
\begin{align}
	 A_{\bomega_\mi,\dummy}(\ba_C) &\leq\, \frac{1}{\eta}\, A_{\bomega_\mi,\dummyx}(\ba_C)\;,\label{eq:states-operators-3a}\\
 A_{\bomega_\mi,x}(\ba_C) &\leq\, \frac{1}{1-\eta}\, A_{\bomega_\mi,\dummyx}(\ba_C)\;.\label{eq:states-operators-3b}
\end{align}



\paragraph{States.}  For all $i \in [n] \setminus C$, $\br_\mi = (\bomega_\mi,\ba_C,\bb_C)$ and $x \in \X$, for all $s \in \{x,\dummyx\}$, and for all $y \in \Y$, define the (unnormalized) state
\begin{equation}\label{eq:def-state-y}
	\ket{\Phi_{\br_\mi,s,y}} = A_{\bomega_\mi,s}(\ba_C)^{1/2} \otimes B_{\bomega_\mi,y}(\bb_C)^{1/2} \, \ket{\psi}~.
\end{equation}
and define the normalization factor
\begin{equation}\label{eq:def-gamma}
	\gamma_{\br_\mi,s,y} = \big \| \, \ket{\Phi_{\br_\mi,s,y}} \, \big \|~.
\end{equation}

The following proposition expresses the normalization factors $\gamma_{\br_\mi,s,y}$ as a function of the probability of obtaining answers $(\ba_C,\bb_C)$ in the strategy $\strategy^n$. 

\begin{proposition}
\label{prop:gamma}
For all $s \in \X$, 
\begin{equation}\label{eq:gamma-def-1}
	\gamma_{\br_\mi,s,y} = \Big( \P_{\bA_C \bB_C | \bOmega_\mi = \bomega_\mi, \bX_i = s, \bY_i = y}(\ba_C,\bb_C) \Big)^{1/2}\;,
\end{equation}
and for all $s = \dummyx \in \X_{/\perp}$,
\begin{align}\label{eq:gamma-def-2}
	\gamma_{\br_\mi,s,y} &= \Big( \eta \, \P_{\bA_C \bB_C | \bOmega_\mi = \bomega_\mi, \bX_i = \dummy, \bY_i = y}(\ba_C,\bb_C) + (1 - \eta) \, \P_{\bA_C \bB_C | \bOmega_\mi = \bomega_\mi, \bX_i = x, \bY_i = y}(\ba_C,\bb_C)\Big)^{1/2} \\
	&= \P_{\bA_C \bB_C | \bOmega_\mi = \bomega_\mi, \bOmega_i = (A,x), \bY_i = y}(\ba_C,\bb_C)^{1/2}~.\label{eq:gamma-def-3}
\end{align}
\end{proposition}

\begin{proof}
Suppose that $s \in \X$. Expanding the definition of $ A_{\bomega_\mi,s}(\ba_C)$ and $B_{\bomega_\mi,y}(\bb_C)$,
\begin{align*}
	\gamma_{\br_\mi,s,y}^2 &= \bra{\psi} A_{\bomega_\mi,s}(\ba_C) \otimes B_{\bomega_\mi,y}(\bb_C) \, \ket{\psi} \\
	&= \Ex_{\bX | \bomega_\mi, s} \, \Ex_{\bY | \bomega_\mi, y} \, \bra{\psi} A_\bx(\ba_C) \otimes B_\by(\bb_C) \, \ket{\psi} \\
	&= \Ex_{\bX | \bomega_\mi, s} \, \Ex_{\bY | \bomega_\mi, y} \, \bra{\psi} A_\bx(\ba_C) \otimes B_\by(\bb_C) \, \ket{\psi} \\
	&= \Ex_{\bX, \bY | \bOmega_\mi = \bomega_\mi, \bX_i = s, \bY_i = y} \, \P_{\bA_C \bB_C | \bx, \by}(\ba_C,\bb_C) \\
	&= \P_{\bA_C \bB_C | \bOmega_\mi = \bomega_\mi, \bX_i = s,\bY_i = y}(\ba_C,\bb_C)~.
\end{align*}
In the second-to-last line, we used the fact that $\bX$ and $\bY$ are independent conditioned on $\bOmega_\mi$. 
The calculation for when $s = \dummyx$ follows in nearly an identical manner to establish~\eqref{eq:gamma-def-2}; the equality in~\eqref{eq:gamma-def-3} follows from how $\bX_i$ and $\bOmega_i$ are coupled together (see \Cref{sec:p_setup}). 
\end{proof}

Let
\begin{equation}\label{eq:def-psitt}
	\ket{\wt{\Phi}_{\br_\mi,s,y}} = \gamma_{\br_\mi,s,y}^{-1} \, \ket{\Phi_{\br_\mi,s,y}}
\end{equation}
denote the normalized version of the state $\ket{\Phi_{\br_\mi,s,y}}$.

For notational convenience we often suppress the dependence on $i$, $\bomega_\mi$, $\ba_C$, and $\bb_C$ when it is clear from context. Thus, for example, when we refer to an operator such as $A_\dummyx$, we really mean the operator $A_{\bomega_\mi,\dummy\!/\bx_i}(\ba_C)$ where $\bx_i = x$. As another example, we will often write $\ket{\wt{\Phi}_{x,y}}$ to denote the state $\ket{\wt{\Phi}_\ssxy}$.


%For all $x\in \X$, $y \in \Y$, define the following (unnormalized) states: \tnote{I think the definition of the $\dummy/\dummy$ state is missing}

%
%\begin{align}\label{eq:operator-norm-bound}
%	A_\dummyx^{-1/2} A_x A_\dummyx^{-1/2} \preceq \frac{3}{2} \Id \qquad A_\dummyx^{-1/2} A_\dummy A_\dummyx^{-1/2} \preceq 3 \Id 
%\end{align}

\subsection{Proof of the Main Theorem (\Cref{thm:anchorpr_quantum})}

%\begin{lemma}
%\label{lem:anchorpr_main_lemma}
%	Let $G$ be an $\alpha$-anchored two-player game. Let $C \subseteq [n]$ be a set of coordinates. Then 
%	$$
%		\Ex_{i \notin C} \P(W_i | W_C) \leq \eval(G) + O(\delta_C^{1/8}/\alpha^2)
%	$$
%	where the expectation is over a uniformly chosen $i \in [n] \backslash C$ and $\delta_C = \frac{1}{m} \left ( \log \frac{1}{\P(W_C)} + |C| \log |\A| |\B| \right)$.
%\end{lemma}
%\begin{proof}
%Using the same reasoning that allowed us to derive Theorem~\ref{thm:mult_classical} from~\eqref{eq:process_invariant-0}, the proof of Theorem~\ref{thm:anchorpr_quantum} will follow once it is established that for any $C \subseteq [n]$, 
%\begin{equation}\label{eq:process_invariant-q}
%		\Ex_{i \in [n] \backslash C} \Pr(W_i | W_C) \leq \eval(G) + O(\delta^{1/8}/\alpha^2),
%\end{equation}
%where again $\delta = \frac{1}{m} \left ( \log 1/\Pr(W) + |C| \log |\A| |\B| \right)$. The proof of~\eqref{eq:process_invariant-q} 

For every choice of subset $C \subseteq [n]$, index  $i \in [n] \setminus C$, and dependency-breaking variable $\br_\mi = (\bomega_\mi,\ba_C,\bb_C)$ we define a  strategy $\strategy_{\br_\mi}$ for $G$ as follows. The shared state is 
\[\ket{\wt{\Phi}_\sspp} \in \C_{E_A}^d \otimes \C_{E_B}^d\;,\]
as defined in~\eqref{eq:def-psitt}. The measurement operators used by the players are of the form 
\[ A_x(a)\,=\,  U_{\br_\mi,x}^\dagger \what{A}_{\br_\mi,x}(\ba_i)U_{\br_\mi,x}\qquad\text{and}\qquad B_y(b) \,=\,V_{\br_\mi,y}^\dagger\what{B}_{\br_\mi, y}(\bb_i) V_{\br_\mi,y}\;,\]
where $x,y$ denote questions to the first and second player respectively and the answers $a\in \A$, $b\in \B$ are identified with $\ba_i$ and $\bb_i$ on the right-hand side (as will generally be the case in this and the following sections). 
Here, $U_{\br_\mi,x}$, $V_{\br_\mi,y}$ are unitaries and $\{\what{A}_{\br_\mi,x}(\ba_i) \}$ and $\{\what{B}_{\br_\mi, y}(\bb_i)\}$ POVMs that are specified in \Cref{sec:main-lemma-proof}.
Let $\Qsf_{A B | \br_\mi, x,y}$ denote the distribution of outcomes $(a,b)$ on question pair $(x,y)$ using the strategy $\strategy_{\br_\mi}$. %The following Lemma shows that, by picking $(i,\br_\mi)$ randomly, the strategy $\strategy_{\br_\mi}$ generates answers $(a,b)$ that are closely distributed to the distribution of $(\ba_i,\bb_i)$ in the repeated game strategy $\strategy^n$ when we \emph{condition} on the event $W_C$ of winning all the coordinates in $C$:

\begin{lemma}[Main Lemma]
\label{lem:anchorpr_main_lemma}
For all subsets $C \subseteq [n]$, we have that \tnote{Can we take the $I$ out?}
\[
	\big \| \P_I \cdot \P_{\bR_\mi | W_C} \cdot \P_{XY} \cdot \Qsf_{AB | \br_\mi, x,y} - \P_I \cdot \P_{\bX_i \bY_i \bR_\mi \bA_i \bB_i | W_C} \big \| \leq O(\delta^{1/16}/\alpha^3)
\]
where $\delta$ is defined in~\eqref{eq:delta} and we identify $(x,y,a,b)$ with $(\bx_i,\by_i,\ba_i,\bb_i)$. 
\end{lemma}

Before proving \Cref{lem:anchorpr_main_lemma} we show that it implies \Cref{thm:anchorpr_quantum}. We start with a proposition.

\begin{proposition}
\label{prop:subset}
	Let $W$ denote the indicator for winning all $n$ coordinates. Suppose that $n \geq \frac{16}{\eps} \log \frac{4}{\eps \cdot \P(W)}$. Then there exists a set $C \subseteq [n]$ of size at most $t = \frac{8}{\eps} \log \frac{4}{\eps \cdot \P(W)}$ such that
	$$
		\P (W_i | W_C) \geq 1 - \eps/2.
	$$
	where $i$ is chosen uniformly from $[n] \setminus C$\tnote{what does it mean? The average probability is large? Or it's large with high probability for such $i$? For all such $i$?}. Here, $\P(W_i | W_C)$ denotes the probability of winning the $i$-th coordinate, conditioned on winning all the $C$-coordinates\tnote{What's a $C$-coordinate??}.
\end{proposition}

\begin{proof}
	Set $\delta = \eps/8$. Let $W_{> 1 - \delta}$ denote the event that the players win more than $(1 - \delta)n$ rounds\tnote{Is this the strategy $\strategy^n$? Also, what's a round?}. To show existence of such a set $C$, we will show that $\Ex_C \P(\neg W_i | W_C) \leq \eps/2$, where $C$ is a (multi)set of $t$ independently chosen indices in $[n]$\tnote{what about $i$?}. This implies that there exists a particular set $C$ such that $\P(\neg W_i | W_C) \leq \eps/2$, which concludes the claim.
	
	First we write, for a fixed $C$,
	\begin{align*}
		\P ( \neg W_i | W_C) = \P(\neg W_i | W_C, W_{> 1 - \delta}) \P(W_{> 1 - \delta} | W_C) + \P(\neg W_i | W_C, \neg W_{> 1 - \delta}) \P(\neg W_{> 1 - \delta} | W_C).
	\end{align*}
	Observe that $\P(\neg W_i | W_C \wedge W_{> 1 - \delta})$ is the probability that, conditioned on winning all rounds in $C$, the randomly selected coordinate $i \in [n] \setminus C$ happens to be one of the (at most) $\delta n$ lost rounds. This is at most $\delta n/(n - t) \leq \eps/4$, where we use our assumption on $t$ from the Proposition statement. Now observe that 
	\begin{align*}
		\Ex_C \P(\neg W_{> 1 - \delta} | W_C) \leq \Ex_C \frac{\P(W_C | \neg W_{> 1 - \delta})}{\P(W_C)} \leq \frac{1}{\P(W)} (1 - \delta)^t \leq \eps/4
	\end{align*}
	where in the second line we used the fact that $\P(W_C) \geq \P(W)$.	
\end{proof}

The lower bound on $\P(W)$ given by Eq.~\eqref{eq:p} satisfies the condition of \Cref{prop:subset}\tnote{Is this clear? I'm wondering about the role of $s$ in~\eqref{eq:p}}; fix a subset $C \subseteq [n]$ satisfying the conclusions of the Proposition. Then sampling from the probability distribution $\P_I \cdot \P_{\bX_i \bY_i \bR_\mi \bA_i \bB_i | W_C}$ yields a tuple $(i,\bx_i,\by_i,\br_\mi,\ba_i,\bb_i)$ such that $V(\bx_i,\by_i,\ba_i,\bb_i) = 1$ (i.e., $W_i = 1$) with probability at least $1 - \eps/2$. On the other hand, \Cref{lem:anchorpr_main_lemma} implies that if we were to first sample $(i,\br_\mi)$ from the distribution $\P_I \cdot \P_{\bR_\mi | W_C}$ and then play the game $G$ using strategy $\strategy_{\br_\mi}$, we obtain a distribution over tuples $(i,\br_\mi,x,y,a,b)$ that is $O(\delta^{1/16}/\alpha^3)$-close to $\P_I \cdot \P_{\bX_i \bY_i \bR_\mi \bA_i \bB_i | W_C}$. In particular, this yields winning answers with probability at least $1 - \eps/2 - O(\delta^{1/16}/\alpha^3)$. 

By construction, $t \leq n/2$ so it holds that
\[
\delta = \frac{1}{n-t} \Big( \log \frac{1}{\P(W_C)} + t \cdot s \Big) \leq \frac{2}{n} \Big ( \frac{16 \cdot s}{\eps} \log \frac{4}{\eps \cdot \P(W)} \Big) \leq O \Big( \frac{\alpha^{48} \cdot \eps^{16}}{4^{16}} \Big)
\]
where $s = \log |\A \times \B|$. Thus $O(\delta^{1/16}/\alpha^3) \leq \eps/4$, meaning that the probability that the strategy $\strategy_{\br_\mi}$ wins $G$ is at least $1 - \eps$.

Furthermore, by an averaging argument, there must exist a pair $(i,\br_\mi)$ such that
\[
	\eval(G,\strategy_{\br_\mi}) \geq 1 - \eps.
\]
Since $\strategy_{\br_\mi}$ is a $d$-dimensional strategy where $d = \E(G^n,p)$, this implies that $\E(G,1 - \eps) \leq d$, which concludes the proof of \Cref{thm:anchorpr_quantum}.

\subsection{Proof of \Cref{lem:anchorpr_main_lemma}}
\label{sec:main-lemma-proof}

Fix a subset $C \subseteq [n]$ and let $m=n-|C|$. Fix an $i\in [n]\backslash C$. For any $\br_\mi,x,y$ and $a,b$ define positive semidefinite operators
\begin{align*}
	\what{A}_{\br_\mi,x}(a) = \sum_{\ba | a, \ba_C} (A_{\bomega_\mi,x}(\ba_C) )^{-1/2} \cdot A_{\bomega_\mi,x}(\ba) \cdot (A_{\bomega_\mi,x}(\ba_C) )^{-1/2} \;,\\
	\what{B}_{\br_\mi, y}(b) = \sum_{\bb | b, \bb_C}  (B_{\bomega_\mi,y}(\bb_C) )^{-1/2} \cdot B_{\bomega_\mi,y}(\bb) \cdot (B_{\bomega_\mi,y}(\bb_C) )^{-1/2}\;,
\end{align*}
where $\ba | a, \ba_C$ (resp. $\bb | b, \bb_C$) denotes summing over tuples $\ba$ that are consistent with $\ba_C$ and $\ba_i = a$ (resp. $\bb$ that are consistent with $\bb_C$ and $\bb_i = b$). For fixed $\br_\mi, x,y$ the collections $\{\what{A}_{\br_\mi, x}(a) \}_{a}$ and $\{\what{B}_{\br_\mi,y}(b)\}_{b}$ are valid POVMs. 

The following lemma specifies the existence of unitaries $U_{\br_\mi,x}$ and $V_{\br_\mi,y}$ used to define the stategy $\strategy$. Recall the definition of the states $\ket{\wt{\Phi}_{\br_\mi,s,y}}$, for $s\in $ and $y\in \Y$, in~\eqref{eq:def-psitt}.
	

\begin{lemma}
\label{lem:local_unitaries}
	For every $C$, $i$, $\br_\mi$, $x$ and $y$, there exists unitaries $U_{\br_\mi,x}$ acting on $E_A$ and $V_{\br_\mi,y}$ acting on $E_B$ such that
	$$
		\Ex_I \,\, \Ex_{\bR_\mi | W_C} \,\, \Ex_{XY}  \Big \| (U_{\br_\mi,x} \otimes V_{\br_\mi,y}) \bigket{\wt{\Phi}_{\sspp}} - \bigket{\wt{\Phi}_{\ssxy}} \Big\| = O(\delta^{1/16}/\alpha^3)\;,
	$$
	where $\Ex_I$ denotes the expectation over a uniformly random $i \in [n] \setminus C$, $\Ex_{\bR_\mi | W_C}$ denotes the expectation over $\br_\mi$ sampled from $\P_{\bR_\mi | W_C}$, and $\Ex_{XY}$ denotes the expectation over $(x,y)$ sampled from $\mu$. 
\end{lemma}

The proof of \Cref{lem:local_unitaries} is given in Section~\ref{sec:anchorpr_main_lemma}. 
%Thus after applying the unitaries in strategy $\strategy_{\br_\mi}$, the players' shared state is $\beta_{\br_\mi,x,y}$-close to the state $\ket{\wt{\Phi}_{\ssxy}}$. When the players measure this state using the POVMs $\{\what{A}_{\br_\mi,x}(a) \}$ and $\{\what{B}_{\br_\mi, y}(b)\}$, they obtain outcomes $(a,b)$ with probability
%\begin{equation}
%\label{eq:anchorpr_main-1}
%	\Tr \left ( \what{A}_{\br_\mi, x}(a) \otimes \what{B}_{\br_\mi, y}(b) \,\,\, \Big( U_{\br_\mi,x} \otimes V_{\br_\mi,y} \Big) \wt{\Phi}_{\sspp} \Big( U_{\br_\mi,x} \otimes V_{\br_\mi,y} \Big)^\dagger \right )
%\end{equation}
%Suppose that the players measure the state $\ket{\wt{\Phi}_{\ssxy}}$ instead of the state $U_{\br_\mi,x} \otimes V_{\br_\mi,y} \ket{\wt{\Phi}_\sspp}$. 
%The players obtain the outcomes $(a,b)$ with probability

\begin{claim}\label{claim:main-1}\tnote{This was some kind of free-flowing discussion; I made it a claim}
For all $\br_\mi, x,y$ and $a,b$,
\begin{equation}\label{eq:anchorpr_main-2}
\Tr \big ( \what{A}_{\br_\mi, x}(a) \otimes \what{B}_{\br_\mi, y}(b) \,\,\,\wt{\Phi}_{\ssxy} \big )  \,=\,\P_{\bA_i \bB_i | \br_\mi,x,y}(a,b)\;.
\end{equation}
\end{claim}

\begin{proof}
By definition of $\wt{\Phi}_{\ssxy}$ the left-hand side of~\eqref{eq:anchorpr_main-2} can be expanded as
\begin{align*}
\Tr \big ( \what{A}_{\br_\mi, x}(a) \otimes \what{B}_{\br_\mi, y}(b) \,\,\,\wt{\Phi}_{\ssxy} \big )	&= \gamma_\ssxy^{-2} \cdot \sum_{\substack{\ba | a, \ba_C \\ \bb | b, \bb_C}}  \Tr \left ( A_{\bomega_\mi,x}(\ba) \otimes B_{\bomega_\mi,y}(\bb) \,\, \ketbra{\psi}{\psi} \right ) \notag \\
	&= \P_{\bA_C \bB_C|\bomega_\mi, x, y}(\ba_C,\bb_C)^{-1} \cdot \sum_{\substack{\ba | a, \ba_C \\ \bb | b, \bb_C}}  \Tr \left ( A_{\bomega_\mi,x}(\ba) \otimes B_{\bomega_\mi,y}(\bb) \,\, \ketbra{\psi}{\psi} \right ) \notag \\
	&= 	\P_{\bA_C \bB_C|\bomega_\mi, x,y}(\ba_C,\bb_C)^{-1} \cdot \Ex_{\substack{\bX| \bomega_\mi, x \\ \bY| \bomega_\mi ,y}} \sum_{\substack{\ba | a, \ba_C \\ \bb | b, \bb_C}}  \Tr \left ( A_{\bx}(\ba) \otimes B_{\by}(\bb) \, \ketbra{\psi}{\psi} \right )\notag \\
	&= 	\P_{\bA_C \bB_C|\bomega_\mi, x, y}(\ba_C,\bb_C)^{-1} \cdot \Ex_{\bX \bY | \bomega_\mi ,x, y} \sum_{\substack{\ba | a, \ba_C \\ \bb | b, \bb_C}}  \Tr \left ( A_{\bx}(\ba) \otimes B_{\by}(\bb) \, \ketbra{\psi}{\psi} \right ) \notag \\	
	&= \P_{\bA_C \bB_C|\bomega_\mi ,x,y}(\ba_C,\bb_C)^{-1} \cdot \P_{\bA_C\bB_C\bA_i \bB_i | \bomega_\mi,x,y}(\ba_C,\bb_C,a,b) \notag \\
	&= \P_{\bA_i \bB_i | \br_\mi,x,y}(a,b)\;,\notag
\end{align*}
where the second line is by~\eqref{eq:gamma-def-1}, the third uses the definition  of $A_{\bomega_\mi,x}(\ba)$, $B_{\bomega_\mi,y}(\bb)$ in~\eqref{eq:states-operators-2}, and the fourth uses that by Claim~\ref{clm:dependency-breaking-1}\tnote{added reference to claim, and to definitions; correct? It would really help if you would do such things, rather than stating facts shown 5 pages earlier without explaining where the fact comes from} $\bX, \bY$ are independent conditioned on $\bomega_\mi$, $\bX_i = x$ and $\bY_i = y$.
\end{proof}

%Let $\mathcal{C}_{\br_\mi,x,y}(\cdot)$ denote the completely positive trace preserving (CPTP) map that measures an input state $\rho$ using the POVM $\{\what{A}_{\br_\mi,x}(a) \otimes \what{B}_{\br_\mi,y}(b) \}_{a,b}$, and outputs $(a,b) \in \A \times \B$. Thus, for fixed $\br_\mi, x,y$, 
\begin{claim}\label{claim:main-2}\tnote{same comment}
The following holds:
\begin{equation}\label{eq:main-2-0}
\Ex_I \big\| \P_{\bR_\mi |W_C} \cdot \P_{\bX_i \bY_i} \cdot \Qsf_{\bA_i \bB_i |\bR_\mi \bX_i \bY_i } 
	-  \P_{\bR_\mi |W_C} \cdot \P_{\bX_i \bY_i} \cdot \P_{\bA_i \bB_i | \bR_\mi \bX_i \bY_i} \big \|  
\,=\,O(\delta^{1/16}/\alpha^3)\;.
\end{equation}
\end{claim}

\begin{proof}
Fix $\br_\mi, x,y$. We bound the total variation distance
\begin{align}
	\big \| \Qsf_{\bA_i \bB_i | \br_\mi, x,y} - \P_{\bA_i \bB_i | \br_\mi,x,y}\big \|
	&\leq  \big \|  \big (   U_{\br_\mi,x} \otimes V_{\br_\mi,y} \big) \wt{\Phi}_{\sspp} \big (   U_{\br_\mi,x} \otimes V_{\br_\mi,y} \big)^\dagger -  \wt{\Phi}_{\br_\mi,x,y}   \big \|_1 \notag\\
	&\leq \sqrt{2} \,\big\| U_{\br_\mi,x} \otimes V_{\br_\mi,y} \ket{\wt{\Phi}_\sspp} - \ket{\wt{\Phi}_{\ssxy}} \big\|\;.\label{eq:main-2-1}
\end{align}
Here the first line follows by contractivity of the trace distance, since using~\eqref{eq:anchorpr_main-2} for $P$ and the definition for $Q$
each distribution on the left-hand side is obtained by measuring the corresponding state on the right-hand side using the POVM $\{\what{A}_{\br_\mi, x}(a) \otimes \what{B}_{\br_\mi, y}(b)\}_{a,b}$. The second line follows from the fact that for  pure states $\ket{\psi}$ and $\ket{\phi}$, $ \| \psi - \phi \|_1 \leq \sqrt{2} \|\, \ket{\psi} - \ket{\phi} \|$.
Thus
\begin{align*}
\Ex_I \,\, \big\| \P_{\bR_\mi |W_C} \cdot \P_{\bX_i \bY_i} \cdot \Qsf_{\bA_i \bB_i |\bR_\mi \bX_i \bY_i } 
	&-  \P_{\bR_\mi |W_C} \cdot \P_{\bX_i \bY_i} \cdot \P_{\bA_i \bB_i | \bR_\mi \bX_i \bY_i} \big \|  \\
	&= \Ex_I \, \, \Ex_{\bR_\mi |W_C} \,\, \Ex_{\bX_i \bY_i} \,\, \big \| \Qsf_{\bA_i \bB_i |\br_\mi \bx_i \by_i} - \P_{\bA_i \bB_i | \br_\mi \bx_i \by_i} \big \|    \\
	&\leq \sqrt{2} \, \Ex_I \, \, \Ex_{\bR_\mi |W_C} \,\, \Ex_{\bX_i \bY_i} \big\| U_{\br_\mi,x} \otimes V_{\br_\mi,y} \ket{\wt{\Phi}_\sspp} - \ket{\wt{\Phi}_{\ssxy}} \big\|\\
%	&= \sqrt{2} \cdot \sqrt{\Ex_I \, \, \Ex_{\bR_\mi |W_C} \,\, \Ex_{\bX_i \bY_i} \beta_{\br_\mi,\bx_i,\by_i}^2} & \text{(Jensen's inequality)}\\
	&\leq O(\delta^{1/16}/\alpha^3)\;,
\end{align*}
where the first inequality is by~\eqref{eq:main-2-1} and the last inequality follows from \Cref{lem:local_unitaries}.
\end{proof}


 Item 2 of \Cref{lem:classical_skew} states that 
\[
\Ex_I \left \| \P_{\bR_\mi |W_C} \cdot \P_{\bX_i \bY_i} - \P_{\bR_\mi \bX_i \bY_i | W_C} \right \| \leq \sqrt{\delta}~.
\] 
Starting from~\eqref{eq:main-2-0} and using the preceding inequality together with Lemma~\ref{lem:trivial}\tnote{you had written ``triangle inequality'', but it's more the lemma, no?} we get
\[
\Ex_I \left \|  \P_{\bR_\mi |W_C} \cdot \P_{\bX_i \bY_i} \cdot \Qsf_{\bA_i \bB_i |\bR_\mi \bX_i \bY_i } 
	-  \P_{\bR_\mi \bX_i \bY_i \bA_i \bB_i |W_C}  \right \| \,=\, O\big(\delta^{1/16}/\alpha^3\big)\;.
\]
This concludes the proof of \Cref{lem:anchorpr_main_lemma}.

\subsection{Proof of \Cref{lem:local_unitaries}}
\label{sec:anchorpr_main_lemma}

This section is devoted to the proof of Lemma~\ref{lem:local_unitaries}. The proof is based on two lemmas. The first defines the unitaries $U_{\br_\mi,x}$, $V_{\br_\mi,y}$ as well as additional unitaries $V_{\br_\mi,x,y}$ that are used in the proof. %The states $\ket{\wt{\Phi}_\ssxp}$, etc., are defined in \Cref{sec:states-operators}.

\begin{lemma}
\label{lem:unitary_bounds}
For all $i$, $\br_\mi$, $x$ and $y$ there exists unitaries $U_{\br_\mi, x}$ acting on $E_A$ and unitaries $V_{\br_\mi, y}$, $V_{\ssxy}$ acting on $E_B$ such that with probability at least $1 - O(\delta^{1/16})$ over the choice of index $i \in [n] \setminus C$, we have
\begin{align}
	\Ex_{\bR_\mi | W_C} \,\, \Ex_X \,\,\,\, \, \, \left \|(U_{\br_\mi, x} \otimes \Id)  \bigket{\wt{\Phi}_\sspp} - \bigket{\wt{\Phi}_\ssxp }   \right \| &=  O(\delta^{1/16}/\alpha^{5/4})\;,\label{eq:ux_bound}\\
	\Ex_{\bR_\mi | W_C} \,\, \Ex_Y \,\, \,\, \, \, \left \| (\Id \otimes V_{\br_\mi,y})  \bigket{\wt{\Phi}_\sspp}  - \bigket{\wt{\Phi}_\sspy }  \right \| &=  O(\delta^{1/16}/\alpha^{5/4})\;,\label{eq:vy_bound}\\
	\Ex_{\bR_\mi | W_C} \,\, \Ex_{XY} \,\,  \left \|  (\Id \otimes V_{\ssxy})  \bigket{\wt{\Phi}_\sspxy}  - \bigket{\wt{\Phi}_\sspxp }  \right \| &=  O(\delta^{1/16}/\alpha^{5/4})\;.\label{eq:vxy_bound}
		\end{align}
		where $\Ex_X$, $\Ex_Y$, and $\Ex_{XY}$ denote expectations under $\mu_X(x)$, $\mu_Y(y)$, and $\mu_{XY}(x,y)$ respectively.
\end{lemma}

The proof of Lemma~\ref{lem:unitary_bounds} is given in Section~\ref{sec:local-unitaries}.
The second lemma relates the normalization factors $\gamma_\xy$ and $\gamma_{\pxy}$ defined in~\eqref{eq:def-gamma}. Recall that according to Proposition~\ref{prop:gamma} these normalization factors, when squared, correspond to the probabilities of obtaining outcomes $(\ba_C,\bb_C)$ conditioned on the dependency-breaking variable $\bomega_\mi$ and setting of the inputs $\bx_i,\by_i$ determined by the subscript of $\gamma$.

\begin{lemma}\label{lem:norms-are-close}
With probability at least $1 - O(\delta^{1/4})$ over the choice of $i \in [n] \setminus C$, we have
	\begin{equation}
	\label{eq:norms-are-close-s0}
		\Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Big | 1 - \frac{\gamma_{\br_\mi, x,y}}{\gamma_{\br_\mi,\dummy,\dummy}} \Big |^2 \leq O(\delta^{1/4}/\alpha^2)\;,
	\end{equation}
	and
	\begin{equation}
	\label{eq:norms-are-close-s1}
		\Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Big | 1 - \frac{\gamma_\sspxy}{\gamma_{\br_\mi,\dummy,\dummy}} \Big |^2 \leq O(\delta^{1/4}/\alpha^3)\;,
	\end{equation}
	where the expectation over the random variable $\bR_\mi$ is conditioned on the events $\bX_i = \dummy$, $\bY_i = \dummy$, and $W_C$. 
\end{lemma}
Before proceeding with the proof we note that the denominator, $\gamma_\sspp$, cannot be zero in the expectation. This is because if $\br_\mi$ is sampled according to $\P_{\bR_\mi | \dummy, \dummy, W_C}$ with positive probability then $\P_{\bA_C \bB_C | \bomega_\mi, W_C}(\ba_C,\bb_C)$ must be nonzero. 

\begin{proof}
We first establish~\eqref{eq:norms-are-close-s0}. For this proof only we introduce the shorthand notation $\br_\mi = (\bomega_\mi,\ba_C,\bb_C) \in W_C$ to signify that the tuple $(\bx_C,\by_C,\ba_C,\bb_C)$ implies the event $W_C$, i.e.\ the tuple corresponds to winning questions and answers in the coordinates indexed by $C$). We start with the following claim.

\begin{claim}
With probability at least $1 - O(\delta^{1/4}/\alpha^2)$ over the choice of $i \in [n] \setminus C$,
\begin{equation}
\label{eq:norms-are-close-0}
\Ex_{XY} \sum_{\br_\mi \in W_C}  \Big | \P_{\bR_\mi | x,y}(\br_\mi) - \P_{\bR_\mi | \dummy, \dummy}(\br_\mi) \Big | = O\Big(\frac{\delta^{1/4}}{\alpha^2}\Big) \P(W_C | \bX_i = \dummy, \bY_i = \dummy)\;.
\end{equation}
\end{claim}

\begin{proof}
First note that
\begin{align}
\nonumber
\Ex_I \sum_{x,y} \P_{XY}(x,y) \left | \P (W_C| \bX_i = x,\bY_i = y)-\P(W_C) \right |
&= \P(W_C) \Ex_I  \left \| \P_{\bX_i \bY_i |W_C} - \P_{\bX_i\bY_i} \right \| \\
\label{eq:27-0}
&= O(\sqrt{\delta}) \cdot \P(W_C)\;,
\end{align}
where the second equality follows from the Item 1 of Lemma~\ref{lem:classical_skew}. Using $\P_{XY}(\bX_i = \dummy, \bY_i = \dummy) \geq \alpha^2$ we get that
\begin{equation}
\label{eq:w_c-doesnt-change}
\Ex_I \left | \P (W_C| \bX_i = \dummy,\bY_i = \dummy)-\P(W_C) \right | \leq O(\sqrt{\delta}/\alpha^2) \cdot \P(W_C)\;.
\end{equation}
Using the triangle inequality with~\eqref{eq:27-0} and~\eqref{eq:w_c-doesnt-change},
\begin{equation}\label{eq:27-1}
\Ex_I \, \Ex_{XY} \Big | \P(W_C| \bX_i = x,\bY_i = y)-\P(W_C | \bX_i = \dummy, \bY_i = \dummy) \Big | = O(\sqrt{\delta}/\alpha^2) \cdot \P(W_C)\;.
\end{equation}
Using the triangle inequality,  
\begin{align}
\Ex_I \, \Ex_{XY} \, \sum_{\br_\mi \in W_C} &\Big | \P(W_C) \cdot \P_{\bR_\mi | x, y, W_C}(\br_\mi) - \P_{\bR_\mi | x,y} (\br_\mi) \Big |\notag \\
 &\leq \Ex_I \, \Ex_{XY} \, \sum_{\br_\mi \in W_C} \Big | \big(\P(W_C|\bX_i = x,\bY_i = y) - \P(W_C)\big) \cdot \P_{\bR_\mi | x, y, W_C}(\br_\mi)  |\notag\\
&\qquad + \Ex_I \, \Ex_{XY} \, \sum_{\br_\mi \in W_C} \Big |  \P_{\bR_\mi \wedge W_C | x,y}(\br_\mi) - \P_{\bR_\mi | x,y}(\br_\mi)  \Big |  \notag \\
&= O(\sqrt{\delta}) \cdot \P(W_C)\;,\label{eq:27-1b}
\end{align}
where the last equality is obtained by using~\eqref{eq:27-0} to bound the first term and observing that the second is $0$. 
Conditioning on $(X,Y) = (\dummy,\dummy)$ in~\eqref{eq:27-1b} we get
\begin{align}
\Ex_I \, \Ex_{XY} \, \sum_{\br_\mi\in W_C} \Big | \P(W_C) \cdot \P_{\bR_\mi |  \dummy,  \dummy, W_C}(\br_\mi) - \P_{\bR_\mi |\dummy,  \dummy}(\br_\mi)  \Big | = O\Big(\frac{\sqrt{\delta}}{\alpha^2}\Big) \P(W_C)~.\label{eq:27-1c}
\end{align}
Combining~\eqref{eq:27-1b} and~\eqref{eq:27-1c} with Item 3 of Lemma~\ref{lem:classical_skew}, which can be stated as\tnote{I didn't get this. Are you using~\eqref{eq:27-1}? The couple following lines are also a bit fuzzy} 
$$
\P(W_C) \, \Ex_I \, \Ex_{XY} \,\|\P_{\bR_\mi | \dummy, \dummy, W_C} - \P_{\bR_\mi | x,y, W_C} \| \leq O\Big(\frac{\sqrt{\delta}}{\alpha^2}\Big) \P(W_C)
$$ 
we obtain 
$$\Ex_I \Ex_{XY} \sum_{\br_\mi \in W_C}  \Big | \P_{\bR_\mi | x,y}(\br_\mi) - \P_{\bR_\mi | \dummy, \dummy}(\br_\mi) \Big | = O\Big(\frac{\sqrt{\delta}}{\alpha^2}\Big) \P(W_C).$$ 
Using Markov's inequality, with probability at least $1 - \kappa$ over $i \in [n] \setminus C$, we have
\[
\Ex_{XY} \sum_{\br_\mi \in W_C}  \Big | \P_{\bR_\mi | x,y}(\br_\mi) - \P_{\bR_\mi | \dummy, \dummy}(\br_\mi) \Big | = O\Big(\frac{\sqrt{\delta}}{\kappa \cdot \alpha^2}\Big) \P(W_C)
\]
All that remains is to show that with high probability over the index $i$, $\P(W_C)$ is not too far from $\P(W_C | \bX_i = \dummy, \bY_i = \dummy)$. From~\eqref{eq:w_c-doesnt-change} and Markov's inequality we have that with probability at least $1 - \kappa$ over $i \in [n] \setminus C$, we have 
\[
	\Big (1 - O\Big(\frac{\sqrt{\delta}}{\kappa \cdot \alpha^2}\Big) \Big)  \cdot \P(W_C) \leq \P (W_C| \bX_i = \dummy,\bY_i = \dummy) \leq \Big (1 + O\Big(\frac{\sqrt{\delta}}{\kappa \cdot \alpha^2}\Big) \Big) \cdot \P(W_C)~.
\]
Setting $\kappa = \delta^{1/4}/2$, we obtain~\eqref{eq:norms-are-close-0}
\end{proof}

We proceed with the proof of Lemma~\ref{lem:norms-are-close}. 
Observe that we can write
	\begin{align*}
		\P_{\bR_\mi | \dummy, \dummy, W_C}(\br_\mi) &= \P(W_C |  \bX_i = \dummy, \bY_i = \dummy)^{-1} \cdot \P_{\bOmega_\mi \bA_C \bB_C \wedge W_C | \dummy, \dummy}(\bomega_\mi,\ba_C,\bb_C) \\
		&= \P(W_C | \bX_i = \dummy, \bY_i = \dummy)^{-1} \cdot \P_{\bOmega_\mi \bA_C \bB_C | \dummy, \dummy}(\bomega_\mi,\ba_C,\bb_C) \\
		&= \P(W_C | \bX_i = \dummy, \bY_i = \dummy)^{-1} \cdot \P_{\bOmega_\mi | \dummy, \dummy}(\bomega_\mi) \cdot \P_{\bA_C \bB_C | \bomega_\mi,\dummy, \dummy}(\ba_C,\bb_C) \\
		&= \P(W_C |\bX_i = \dummy, \bY_i = \dummy)^{-1} \cdot \P_{\bOmega_\mi}(\bomega_\mi) \cdot \gamma_{\br_\mi,\dummy,\dummy}^2 \;,
	\end{align*}
	where the second line follows from the fact that, given $\br_\mi \in W_C$, the tuple $(\bx_C,\by_C,\ba_C,\bb_C)$ automatically implies the event $W_C$ and the last line follows from the fact that $\P_{\bOmega_\mi | \bX_i,\bY_i} = \P_{\bOmega_\mi}$, because coordinates of $\bOmega$ are independent.
	
	Fix $i \in [n] \setminus C$. Using that $|a - b|^2 \leq |a^2 - b^2|$ for all $a,b \geq 0$, 
	\begin{align}
		&\Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Big | 1 - \frac{\gamma_{\br_\mi, x,y}}{\gamma_{\br_\mi,\dummy,\dummy}} \Big |^2 \notag \\
		&\leq \Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Big | 1 - \frac{\gamma_{\br_\mi, x,y}^2}{\gamma_{\br_\mi,\dummy,\dummy}^2} \Big | \label{eq:nac-1} \\
		&= \P(W_C |\bX_i = \dummy, \bY_i = \dummy)^{-1} \,\Ex_{XY} \,\sum_{\br_\mi \in W_C} \P_{\bOmega_\mi}(\bomega_\mi) \cdot \gamma_{\br_\mi,\dummy,\dummy}^2 \cdot \, \Big | 1 - \frac{\gamma_{\br_\mi, x,y}^2}{\gamma_{\br_\mi,\dummy,\dummy}^2} \Big | \notag \\
		&= \P(W_C |\bX_i = \dummy, \bY_i = \dummy)^{-1} \, \Ex_{XY} \,\sum_{\br_\mi \in W_C} \P_{\bOmega_\mi}(\bomega_\mi) \cdot \, \Big | \gamma_{\br_\mi,\dummy,\dummy}^2 - \gamma_{\br_\mi, x,y}^2 \Big | \notag \\
		&= \P(W_C |\bX_i = \dummy, \bY_i = \dummy)^{-1} \, \Ex_{XY} \,\sum_{\br_\mi \in W_C} \, \Big | \P_{\bR_\mi | \dummy,\dummy}(\br_\mi) - \P_{\bR_\mi | x,y}(\br_\mi) \Big |~. \label{eq:norms-are-close-0a}
	\end{align}
	In the second-to-last line we used the fact that $\P_{\bOmega_\mi}(\bomega_\mi) = \P_{\bOmega_\mi | \bX_i = x, \bY_i = y}(\bomega_\mi)$ for all $x,y$, which again is because coordinates of $\Omega$ are independent.  

Combining~\eqref{eq:norms-are-close-0} with~\eqref{eq:norms-are-close-0a} shows~\eqref{eq:norms-are-close-s0}. To establish~\eqref{eq:norms-are-close-s1} we notice that for all $i, \br_\mi$, we have
\begin{align*}
	\gamma_\sspxy^2 &= \bra{\psi} A_{\bomega_\mi,\dummy/x}(\ba_C) \otimes B_{\bomega_\mi,y}(\bb_C) \ket{\psi} \\
	&= \eta \, \bra{\psi} A_{\bomega_\mi,\dummy}(\ba_C) \otimes B_{\bomega_\mi,y}(\bb_C) \ket{\psi} + (1 - \eta) \, \bra{\psi} A_{\bomega_\mi,x}(\ba_C) \otimes B_{\bomega_\mi,y}(\bb_C) \ket{\psi} \\
	&= \eta \, \gamma_\sspy^2 + (1 - \eta) \,\gamma_\ssxy^2\;,
\end{align*}
where the second line uses the definition~\eqref{eq:states-operators-3}. 
Therefore
\begin{align*}
\Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Big | 1 - \frac{\gamma_{\br_\mi, \dummy/x,y}}{\gamma_{\br_\mi,\dummy,\dummy}} \Big |^2 &\leq \Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Big | 1 - \frac{\gamma_{\br_\mi, \dummy/x,y}^2}{\gamma_{\br_\mi,\dummy,\dummy}^2} \Big | \\
&\leq \Ex_{XY} \, \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \eta \,\Big | 1 - \frac{\gamma_{\br_\mi, \dummy ,y}^2}{\gamma_{\br_\mi,\dummy,\dummy}^2} \Big |+(1 - \eta)\,\Big | 1 - \frac{\gamma_{\br_\mi, x,y}^2}{\gamma_{\br_\mi,\dummy,\dummy}^2} \Big |\;,
\end{align*}
where the second line is by the triangle inequality. 
Using $\P_X(X = \dummy) \geq \alpha$ the two terms in this last expression can be bounded by $O(\delta^{1/4}/\alpha^3)$ using the proof of~\eqref{eq:norms-are-close-s0}, which in fact bounds the stronger quantity~\eqref{eq:nac-1}. This concludes the proof of \Cref{lem:norms-are-close}. 

\end{proof}


\begin{proof}[Proof of Lemma~\ref{lem:local_unitaries}]\tnote{I cleaned up this proof some, but there's minor issues remaining}
For every $i,\br_\mi$, $x$ and $y$ let unitaries $U_{\br_\mi, x}$, $V_{\br_\mi, y}$ and $V_{\ssxy}$ be as in Lemma~\ref{lem:unitary_bounds}. For notational convenience we suppress the dependence on $(i, \br_\mi)$; thus the unitaries $U_x, V_y, V_{x,y}$, the states $\ket{\Phi_{x,y}}$, and their normalizations $\gamma_{x,y}$ all implicitly depend on $i$ and $\br_\mi$. 

We call an index $i \in [n] \setminus C$ \emph{good} if it satisfies (i) the conclusions of \Cref{lem:unitary_bounds}, (ii) the conclusions of \Cref{lem:norms-are-close}, and (iii) it holds that
\begin{equation}\label{eq:good-i-cond}
\left \| \P_{\bR_\mi | \bX_i = \dummy, \bY_i = \dummy,W_C} - \P_{\bR_\mi | W_C} \right \| \,\leq\, O(\delta^{1/2}/\alpha^2)\;.
\end{equation}
By \Cref{lem:unitary_bounds}, \Cref{lem:norms-are-close}, and Item 4 of \Cref{lem:classical_skew} an index $i$ is good with probability at least $1 - O(\delta^{1/16})$\tnote{I couldn't follow this. How do you use Item 4? The bound should get weaker; also it's not exactly the right condition. Can you spell out any Markov?}.

Using the definition we have that 
\begin{align}
\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \,  \left \|\ket{\wt{\Phi}_{x,y}} - \gamma_{\dummy,\dummy}^{-1}\ket{\Phi_{x,y}} \right \| 
&= \Ex_{\bR_\mi | \dummy, \dummy, W_C} \,\Ex_{XY} \, \left|1 - \frac{\gamma_{x,y}}{\gamma_{\dummy,\dummy}} \right| \notag\\
& = O(\delta^{1/8}/\alpha)~,\label{eq:local_unitaries-0}
\end{align}
where the second line is by Jensen's inequality and~\eqref{eq:norms-are-close-s0} in \Cref{lem:norms-are-close}. Similarly,
\begin{align}
\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \,  \left \|\ket{\wt{\Phi}_{\pxy}} - \gamma_{\dummy,\dummy}^{-1}\ket{\Phi_{\pxy}} \right \| &= \Ex_{\bR_\mi | \dummy, \dummy, W_C} \,\Ex_{XY} \, \left|1 - \frac{\gamma_{\pxy}}{\gamma_{\dummy,\dummy}} \right|\notag\\
& = O(\delta^{1/8}/\alpha^{3/2})~,\label{eq:local_unitaries-0b}
\end{align}
by~\eqref{eq:norms-are-close-s1}. We note that in the above division by $\gamma_\pp$ is well-defined because $\br_\mi$ is sampled with positive probability from the distribution $\P_{\bR_\mi | \dummy,\dummy,W_C}$. Using~\eqref{eq:good-i-cond} we get from the bounds in \Cref{lem:unitary_bounds} that \tnote{Couldn't the $\delta^{1/2}/\alpha^2$ error be dominant?}
\begin{align}
	\Ex_{\bR_\mi | \dummy,\dummy,W_C} \,\, \Ex_X \,\, \,\,\, \left \| \bigket{\wt{\Phi}_\xp } - (U_{x} \otimes \Id)  \bigket{\wt{\Phi}_\pp}  \right \| &=  O(\delta^{1/16}/\alpha)\;,\label{eq:ux_bound-2}\\
	\Ex_{\bR_\mi | \dummy,\dummy,W_C} \,\, \Ex_Y \,\, \,\,\, \left \| (\Id \otimes V_{y})  \bigket{\wt{\Phi}_\pp}  - \bigket{\wt{\Phi}_\py }  \right \| &=  O(\delta^{1/16}/\alpha)\;,\label{eq:vy_bound-2}\\
	\Ex_{\bR_\mi | \dummy,\dummy,W_C} \,\, \Ex_{XY} \,\,  \left \|  (\Id \otimes V_{\xy})  \bigket{\wt{\Phi}_\pxy}  - \bigket{\wt{\Phi}_\pxp }  \right \| &=  O(\delta^{1/16}/\alpha)\;.\label{eq:vxy_bound-2}
		\end{align}

The main step in the proof is provided by the following claim. 

\begin{claim}\label{claim:good-i-2}
It holds that
\begin{align}
 \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \, \big \| (U_{x} \otimes V_{y}) \bigket{\wt{\Phi}_{\pp}} - \bigket{\wt{\Phi}_{\xy}} \big\|  
 &\leq \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \, \gamma_\pp^{-1} \, \left \| V_y \ket{{\Phi}_{\dummy,\dummy}} - \ket{{\Phi}_{\dummy,y}} \right \|\label{eq:good-i-2a}\\
 &\qquad + 2\eta^{-1/2} \gamma_\pp^{-1} \,\big \|  V_{x,y} \ket{\Phi_{\dummyx,y}} - \ket{{\Phi}_{\dummyx,\dummy}} \big \| \label{eq:good-i-2b}\\
&  \qquad + \gamma_\pp^{-1} \, \left \|  U_x \ket{{\Phi}_{\dummy,\dummy}} -  \ket{{\Phi}_{x,\dummy}} \right \| + O(\delta^{1/8}/\alpha)\;.\label{eq:good-i-2c}
\end{align}
\end{claim}

\begin{proof}
We start by writing 
\begin{align}
%&\Ex_{\bR_\mi | W_C} \, \Ex_{XY} \, \left \| (U_{x} \otimes V_{y}) \bigket{\wt{\Phi}_{\pp}} - \bigket{\wt{\Phi}_{\xy}} \right\| \notag \\
 \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \,& \big \| (U_{x} \otimes V_{y}) \bigket{\wt{\Phi}_{\pp}} - \bigket{\wt{\Phi}_{\xy}} \big\| \notag \\
&\leq  \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \, \left \| (U_{x} \otimes V_{y}) \bigket{\wt{\Phi}_{\pp}} - \frac{\gamma_\xy}{\gamma_\pp} \bigket{\wt{\Phi}_{\xy}} \right\| +  \left \| \frac{\gamma_\xy}{\gamma_\pp} \bigket{\wt{\Phi}_{\xy}} - \bigket{\wt{\Phi}_{\xy}} \right\| \notag \\
&= \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \, \gamma_\pp^{-1} \left \| (U_{x} \otimes V_{y}) \bigket{{\Phi}_{\pp}} - \bigket{{\Phi}_{\xy}} \right\| +  \left | \frac{\gamma_\xy}{\gamma_\pp} -1  \right| \notag \\
&\leq \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_{XY} \, \gamma_\pp^{-1} \left \| (U_{x} \otimes V_{y}) \bigket{{\Phi}_{\pp}} - \bigket{{\Phi}_{\xy}} \right\| + O(\delta^{1/8}/\alpha^2) \;,\label{eq:local_unitaries-1}
\end{align}
\tnote{last exponent above was $1/16$, but I think it's $1/8$}
where the last line follows from~\eqref{eq:local_unitaries-0}.

For $s \in \{\dummy, \dummyx, x\}$ and $y \in \Y$ recall that the unnormalized state $\ket{\Phi_{s,y}}$ is defined in~\eqref{eq:def-state-y} as $A_s^{1/2} \otimes B_y^{1/2} \ket{\psi}$. Using~\eqref{eq:states-operators-3a} (resp.~\eqref{eq:states-operators-3b}) it follows that $A_x A_{\dummyx}^{-1/2} A_\dummyx^{1/2} = A_x$ (resp. $A_\dummy A_{\dummyx}^{-1/2} A_\dummyx^{1/2} = A_\dummy$), because the image of $A_x$ (resp. $A_\dummy$) is contained in the image of $A_\dummyx$. Thus
\begin{align}
	&\left \| U_x \ket{\Phi_{\dummy,y}} - \ket{\Phi_{x,y}} \right \| \notag \\
	&= \left \| U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \ket{\Phi_{\dummyx,y}} - A_{x}^{1/2} A_{\dummy\! /x}^{-1/2}  \ket{\Phi_{\dummyx,y}} \right \| \notag \\
	&= \left \| U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \otimes V_{x,y} \ket{\Phi_{\dummyx,y}} - A_{x}^{1/2} A_{\dummy\! /x}^{-1/2} \otimes V_{x,y}  \ket{\Phi_{\dummyx,y}} \right \| \notag \\
&\leq  \left \| \left ( U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \right ) \otimes V_{x,y} \ket{\Phi_{\dummyx,y}} - \left ( U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \right ) \ket{\Phi_{\dummyx, \dummy}} \right \|  \label{eq:22-2a}\\
& \qquad +  \left \| \left ( U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \right ) \ket{\Phi_{\dummyx,\dummy}} - A_x^{1/2} A_\dummyx^{-1/2}  \ket{\Phi_{\dummyx,\dummy}} \right \| \label{eq:22-2b}\\
& \qquad + \left \| A_x^{1/2} A_\dummyx^{-1/2} \ket{\Phi_{\dummyx,\dummy}} - A_x^{1/2} A_\dummyx^{-1/2}  \otimes V_{x,y} \ket{\Phi_{\dummyx,y}} \right \|~,\label{eq:22-2c}
\end{align}
by the triangle inequality. We bound each of these three terms as follows. 
Using $\|A_\dummy^{1/2} A_\dummyx^{-1/2}\|\leq \eta^{-1/2}$, which follows from~\eqref{eq:states-operators-3a} and operator motonicity of the square root, the term~\eqref{eq:22-2a} can be bounded as
$$
\left \| \left ( U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \right ) \otimes V_{x,y}\, \ket{\Phi_{\dummyx,y}} - \left ( U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \right ) \ket{\Phi_{\dummyx,\dummy}} \right \|  \leq \eta^{-1/2} \, \left \|  V_{x,y} \ket{\Phi_{\dummyx,y}} - \ket{\Phi_{\dummyx,\dummy}} \right \|.
$$
The term~\eqref{eq:22-2b} can be re-written as
$$
\left \| \left ( U_x A_\dummy^{1/2} A_\dummyx^{-1/2} \right ) \ket{\Phi_{\dummyx,\dummy}} - A_x^{1/2} A_\dummyx^{-1/2}  \ket{\Phi_{\dummyx,\dummy}} \right \| = \left \| U_x \ket{\Phi_{\dummy,\dummy}} - \ket{\Phi_{x,\dummy}} \right \|.
$$
Finally, using $\|A_x^{1/2} A_\dummyx^{-1/2}\|\leq \eta^{-1/2}$\tnote{Shouldn't this be $\sqrt{2}$ or something, using $\eta \leq 1/2$? I added~\eqref{eq:states-operators-3b} so that you can refer to it, as above} the term~\eqref{eq:22-2c} can be bounded as
$$
\left \| A_x^{1/2} A_\dummyx^{-1/2} \ket{\Phi_{\dummyx,\dummy}} - A_x^{1/2} A_\dummyx^{-1/2}  \otimes V_{x,y}\, \ket{\Phi_{\dummyx,y}} \right \| \leq \eta^{-1/2} \left \|  \ket{\Phi_{\dummyx,\dummy}} - V_{x,y}\, \ket{\Phi_{\dummyx,y}} \right \|.
$$
Putting the three bounds together, from~~\eqref{eq:22-2c}--\eqref{eq:22-2c} we get
\begin{align}
\left \| U_x \ket{\Phi_{\dummy,y}} - \ket{\Phi_{x,y}} \right \| \leq   2 \eta^{-1/2} \left \|  V_{x,y} \ket{\Phi}_{\dummyx,y} - \ket{{\Phi}_{\dummyx,\dummy}} \right \| + \left \|  U_x \ket{{\Phi}_{\dummy,\dummy}} -  \ket{{\Phi}_{x,\dummy}} \right \| .\label{eq:22-3}
\end{align}
Using the triangle inequality and that $U_x$ is unitary,
\begin{align*}
&\left \| (U_x \otimes V_y) \ket{{\Phi}_{\dummy,\dummy}} - \ket{{\Phi}_{x,y}} \right\| \\
&\leq  \left \| V_y \ket{{\Phi}_{\dummy,\dummy}} - \ket{{\Phi}_{\dummy,y}} \right \| + \left \| U_x \ket{{\Phi}_{\dummy,y}} - \ket{{\Phi}_{x,y}} \right \| \notag\\ 
&\leq  \left \| V_y \ket{{\Phi}_{\dummy,\dummy}} - \ket{{\Phi}_{\dummy,y}} \right \|  + 2\eta^{-1/2} \left \|  V_{x,y} \ket{\Phi_{\dummyx,y}} - \ket{{\Phi}_{\dummyx,\dummy}} \right \| +  \left \|  U_x \ket{{\Phi}_{\dummy,\dummy}} -  \ket{{\Phi}_{x,\dummy}} \right \|\;,
\end{align*}
where the last inequality is due to~\eqref{eq:22-3}. Inserting into~\eqref{eq:local_unitaries-1} proves the claim. 
\end{proof}

To conclude the lemma it remains to bound each of the three terms on the right-hand side of Claim~\ref{claim:good-i-2}, observing that using~\eqref{eq:good-i-cond} the expectation $ \Ex_{\bR_\mi | \dummy, \dummy, W_C}$ on the left-hand side can be exchanged with $ \Ex_{\bR_\mi | W_C}$ by introducing an additive $O(\delta^{1/2}/\alpha^2)$ error.

We start with~\eqref{eq:good-i-2c}, writing
\begin{align*}
	\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_X \, \gamma_\pp^{-1} \left \|  U_x \ket{{\Phi}_{\dummy,\dummy}} -  \ket{{\Phi}_{x,\dummy}} \right \| 
	&= 	\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_X \, \left \|  U_x \ket{\wt{\Phi}_{\dummy,\dummy}} -  \frac{\gamma_{\xp}}{\gamma_\pp} \ket{\wt{\Phi}_{x,\dummy}} \right \|\\
	&\leq  \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_X \, \big \|  U_x \ket{\wt{\Phi}_{\dummy,\dummy}} - \ket{\wt{\Phi}_{x,\dummy}} \big \| + \left \| \ket{\wt{\Phi}_{x,\dummy}} - \frac{\gamma_{\xp}}{\gamma_\pp} \ket{\wt{\Phi}_{x,\dummy}}\right \|\\
&= O(\delta^{1/16}/\alpha) + \Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_X \, \Big | 1 - \frac{\gamma_{\xp}}{\gamma_\pp} \Big |~. \\
	&\leq O(\delta^{1/16}/\alpha) + \sqrt{\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_X \, \Big | 1 - \frac{\gamma_{\xp}}{\gamma_\pp} \Big |^2} \;,
	\end{align*}
	where the third line uses~\eqref{eq:ux_bound-2} to bound the first term and the last line is	by Jensen's inequality. Using~\eqref{eq:local_unitaries-0b} this is $O(\delta^{1/16}/\alpha^2)$\tnote{How do you get this?~\eqref{eq:local_unitaries-0b} is about $\dummyx$. Also, it already has some Jensen incorporated}.
The other terms~\eqref{eq:good-i-2a} and~\eqref{eq:good-i-2b} are bounded similarly as\tnote{I think we should at least spell out the triangle inequality, to say what is the intermediate term}
\[
\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_Y \, \gamma_\pp^{-1} \,\, \left \|  V_y \ket{{\Phi}_{\dummy,\dummy}} -  \ket{{\Phi}_\py} \right \| 	\leq O(\delta^{1/16}/\alpha^2)
\]
and
\[
\Ex_{\bR_\mi | \dummy, \dummy, W_C} \, \Ex_Y \, \eta^{-1/2} \, \gamma_\pp^{-1} \,\, \left \|  V_{x,y} \ket{{\Phi}_{\pxy}} -  \ket{{\Phi}_\pxp} \right \| 	\leq O(\delta^{1/16}/\alpha^3)
\]
where we used that $\eta^{-1/2} = O(\alpha^{-1/2})$. 
\end{proof}


\subsection{Proof of Lemma~\ref{lem:unitary_bounds}}\label{sec:local-unitaries}

In this section we prove Lemma~\ref{lem:unitary_bounds}. We first present some results from quantum information theory. For a more comprehensive reference we refer the reader to Wilde's excellent textbook~\cite{wilde2013quantum}.

\subsubsection{Some tools from quantum information theory}

%We use some tools from quantum information theory in our analysis of parallel repetition, specifically in \Cref{sec:local-unitaries}. 
For a Hermitian operator $X$, let $\mathrm{supp}(X)$ denote the projection onto its image. The \emph{relative entropy} between two positive semidefinite operators $\rho$, $\sigma$, denoted by $\D(\rho \| \sigma)$, is defined to be $\Tr(\rho (\log \rho - \log \sigma))$. The \emph{relative min-entropy} $\D_\infty(\rho \| \sigma)$ is defined as $\min\{ \lambda : \rho \preceq 2^\lambda \sigma \}$, and $+\infty$ if this set is empty. Let $\rho^{AB}$ be a bipartite state. The \emph{mutual information} $\I(A:B)_\rho$ between registers $A$ and $B$ in state $\rho$ is defined as $\D(\rho^{AB} \| \rho^A \otimes \rho^B)$. We define the \emph{conditional mutual information} $\I(A:B|C)_\rho$ for a tripartite quantum state $\rho^{ABC}$ as $\I(A:BC)_\rho - \I(A:C)_\rho$. When $\rho$ is classical on $C$, then $\I(A:B|C)_\rho = \Ex_{x \sim \rho^C} \I(A:B)_{\rho_x}$ where $x \sim \rho^C$ denotes sampling $x$ from the classical distribution $\rho^C$.

\begin{proposition}[Theorem 11.8.1 of~\cite{wilde2013quantum}]
\label{prop:relative_entropy_nonneg}
For all density matrices $\rho, \sigma$, the relative entropy $\D(\rho \| \sigma)$ is nonnegative.
\end{proposition}

\begin{proposition}[Strong subadditivity of conditional mutual information, Theorem 11.7.1 of~\cite{wilde2013quantum}]
\label{prop:ssa}
For all tripartite density matrices $\rho^{ABC}$, the conditional mutual information $\I(A:B|C)_\rho$ is nonnegative.
\end{proposition}

\begin{proposition}[Pinsker's inequality, Theorem 11.9.2 of~\cite{wilde2013quantum}]
\label{prop:pinsker}
	For all density matrices $\rho, \sigma$, 
	\[
		 \frac{1}{2 \ln 2} \| \rho - \sigma \|^2_1 \leq \D(\rho \| \sigma)~.
	\]
\end{proposition}


\begin{proposition}[Theorem 11.9.1 in~\cite{wilde2013quantum}]
\label{prop:divergence_data_processing}
Let $\rho^{XY}$ and $\sigma^{XY}$ be quantum states. Then $\D(\rho^{X} \| \sigma^X) \leq \D(\rho^{XY} \| \sigma^{XY})$.
\end{proposition}

\begin{proposition}[Chain rule for relative entropy]
\label{prop:divergence_chain_rule}
	Let $\rho = \sum_x \P(x) \ketbra{x}{x} \otimes \rho_x$ and $\sigma = \sum_x \Qsf(x) \ketbra{x}{x} \otimes \sigma_x$ for some probability distributions $\P$, $\Qsf$. Then $\D(\rho \| \sigma) = \D(\P \| \Qsf) + \Ex_{x \sim \P} \D(\rho_x \| \sigma_x) $, where $\D(\P \| \Qsf) = \sum_x \P(x) \log \frac{\P(x)}{\Qsf(x)}$ denotes the relative entropy between two distributions. In particular, $\D(\rho \| \sigma) \geq \Ex_{x \sim \P} \D(\rho_x \| \sigma_x)$.
\end{proposition}

\begin{proof}
	For all $x$, let $\Pi_x = \mathrm{supp}(\rho_x)$ and $\Gamma_x = \mathrm{supp}(\sigma_x)$. We note that 
	\[
	\log \rho = \sum_x \log \P(x) \ketbra{x}{x} \otimes \Pi_x + \sum_x \ketbra{x}{x} \otimes \log \rho_x~,
	\]
	 and therefore 
	 \[
	 	\rho \log \rho = \sum_x \P(x) \log \P(x) \ketbra{x}{x} \otimes \rho_x + \sum_x \P(x) \ketbra{x}{x} \otimes \rho_x \log \rho_x~.
	\]
	Similarly, 
	\[
		\rho \log \sigma = \sum_x \P(x) \log \Qsf(x) \ketbra{x}{x} \otimes \rho_x \Gamma_x + \sum_x \P(x) \ketbra{x}{x} \otimes \rho_x \log \sigma_x~.
	\] 
	Subtracting, we get
	\begin{align}
		\D(\rho \| \sigma) &= \sum_x \P(x) \Tr \Big( \ketbra{x}{x} \otimes \Big( \log \P(x) \rho_x - \log \Qsf(x) \rho_x \Gamma_x + \rho_x (\log \rho_x - \log \sigma_x) \Big)  \Big ) \notag \\
		&= \sum_x \P(x) \Big( \log \P(x) \, \Tr(\rho_x) - \log \Qsf(x) \, \Tr( \rho_x \Gamma_x ) +  \Tr \Big( \rho_x (\log \rho_x - \log \sigma_x) \Big) \Big) \notag \\
		&= \sum_x \P(x) \Big( \log \P(x)  - \log \Qsf(x) \, \Tr( \rho_x \Gamma_x ) \Big) + \P(x) \, \D(\rho_x \| \sigma_x) \label{eq:divergence_chain_rule}
	\end{align}
	Suppose that for some $x$, $\rho_x \Gamma_x \neq \rho_x$. This implies that the support of $\sigma_x$ does not contain the support of $\rho_x$, and therefore $\D(\rho_x \| \sigma_x) = \infty$. Thus $\D(\P \| \Qsf) + \Ex_{x \sim \P} \D(\rho_x \| \sigma_x) = \infty$ and $\D(\rho \| \sigma) = \infty$ (because each term of the sum in~\eqref{eq:divergence_chain_rule} is nonnegative), establishing equality. Otherwise, $\rho_x \Gamma_x = \rho_x$ for all $x$, and thus~\eqref{eq:divergence_chain_rule} is again equal to $\D(\P \| \Qsf) + \Ex_{x \sim \P} \D(\rho_x \| \sigma_x)$. 
	
	The ``in particular'' statement follows from the fact that $\D(\P \| \Qsf)$ is nonnegative. 
\end{proof}

\begin{proposition}
\label{prop:mutual-information-classical}
Let $\rho^{XE} = \sum_x \P(x) \ketbra{x}{x}^X \otimes \rho_x^E$ denote a classical-quantum state. Then $\I(X:E)_\rho = \Ex_{x \sim \P} \D(\rho_x^E \| \rho^E)$.
\end{proposition}
\begin{proof}
	By definition of mutual information, we have $\I(X : E)_\rho = \D(\rho^{XE} \| \rho^X \otimes \rho^E)$. Letting $\sigma = \sum_x \P(x) \ketbra{x}{x}^X \otimes \rho^E$, we can apply \Cref{prop:divergence_chain_rule} to get
	\[
		\I(X : E)_\rho = \D(\rho^{XE} \| \rho^X \otimes \rho^E) = \Ex_{x \sim \P} \D(\rho_x^E \| \rho^E)~.
	\]
\end{proof}

%\begin{fact}
%\label{fact:divergence_split_rule}
%Let $\rho^{XY}$ and $\sigma^{XY} = \sigma^{X} \otimes \sigma^Y$ be quantum states. Then $\D(\rho^{XY} \| \sigma^{XY} ) \geq \D(\rho^{X} \| \sigma^X) + \D(\rho^{Y} \| \sigma^Y)$.
%\end{fact}

%
%\begin{fact}[\cite{jain2014parallel}, Lemma II.13]
%\label{fact:max_divergence}
%	Let $\rho = p \rho_0 + (1 - p) \rho_1$. Then $\D_\infty(\rho_0 \big \| \rho) \leq \log 1/p$.
%\end{fact}

%\begin{fact}
%\label{fact:relative_min_entropy_contractivity}
%	Let $\rho^{AB}$ and $\sigma^{AB}$ be density matrices. Then $\D_\infty(\rho^{AB} \| \sigma^{AB}) \geq \D_\infty(\rho^A \| \sigma^B)$. 
%\end{fact}
%\begin{proof}
%	Let $\lambda = \D_\infty(\rho^{AB} \| \sigma^{AB})$. Therefore $2^\lambda \sigma^{AB} - \rho^{AB} \succeq 0$ in the positive semidefinite ordering. 
%\end{proof}
%
%\begin{fact}
%\label{fact:relative_min_entropy_chain_rule}
%Let $\rho$, $\sigma$, and $\tau$ be density matrices such that $\D_\infty(\rho \| \sigma) \leq \lambda_1$ and $\D_\infty(\sigma \| \tau) \leq \lambda_2$. Then $\D_\infty(\rho \| \tau) \leq \lambda_1 + \lambda_2$.
%\end{fact}

\begin{proposition}
\label{prop:relative_min_entropy_chain_rule2}
Let $\rho$, $\sigma$, and $\tau$ be density matrices such that $\D(\rho \| \sigma) \leq \lambda_1$ and $\D_\infty(\sigma \| \tau) \leq \lambda_2$. Then $\D_\infty(\rho \| \tau) \leq \lambda_1 + \lambda_2$.
\end{proposition}
\begin{proof}
	$\D_\infty(\sigma \| \tau) = \lambda_2$ implies that $2^{-\lambda_2} \sigma \preceq \tau$. Then,
	\begin{align*}
		\D(\rho \| \tau) &= \Tr(\rho (\log \rho - \log \tau)) \leq \Tr(\rho(\log \rho - \log 2^{-\lambda_2} \sigma)) \\
		&\leq \Tr(\rho(\log \rho - (-\lambda_2)\Id - \log \sigma)) \\
		&\leq \lambda_2 + \Tr(\rho (\log \rho - \log \sigma)) = \lambda_1 + \lambda_2\;.
	\end{align*}
\end{proof}


\begin{proposition}[Quantum Gibbs's Inequality]
\label{prop:divergence_gibbs_inequality}
	For quantum states $\rho^{AB}$, $\sigma^A$, $\tau^B$, it holds that $\D(\rho^{AB} \big \| \sigma^A \otimes \tau^B) \geq \I(A : B)_\rho$.
\end{proposition}
\begin{proof}
	If the images of $\rho^{A}$ and $\rho^B$ are not contained within the images of $\sigma^A$ and $\tau^B$ respectively, then $\D(\rho^{AB} \big \| \sigma^A \otimes \tau^B) = \infty$, and the inequality trivially holds. Otherwise, using the fact that $\log (\sigma \otimes \tau) = (\log \sigma)\otimes \Gamma + \Pi \otimes (\log \tau)$ we have that
	\begin{align*}
	&\D(\rho^{AB} \big \| \sigma^A \otimes \tau^B) - \I(A:B)_\rho \\
	&= \D(\rho^{AB} \big \| \sigma^A \otimes \tau^B) - \D(\rho^{AB} \big \| \rho^A \otimes \rho^B) \\
		&=  \Tr \Big( \rho^{AB} \Big( (\log \rho^A) \otimes \mathrm{supp}(\rho^B) - (\log \sigma^A) \otimes \mathrm{supp}(\tau^B) \Big) \Big) \\
		& \qquad \qquad + \Tr \Big( \rho^{AB} \Big( \mathrm{supp}(\rho^A) \otimes (\log \rho^B)  - \mathrm{supp}(\sigma^A) \otimes (\log \tau^B) \Big) \Big) \\
		&= \D(\rho^A \| \sigma^A) + \D(\rho^B \| \tau^B) \geq 0\;,
	\end{align*}
	where in the last line we used that $\rho^{AB} (\Id \otimes \mathrm{supp}(\tau^B)) = \rho^{AB} (\mathrm{supp}(\sigma^A) \otimes \Id) = \rho^{AB}$, and that the relative entropy is nonnegative (\Cref{prop:relative_entropy_nonneg}). 
\end{proof}

We prove a quantum analogue of \emph{Raz's Lemma}, a central tool used in many proofs of parallel repetition theorems~\cite{raz1998parallel, Hol09,barak2009strong}. 

\begin{lemma}[Quantum Raz's Lemma]
\label{lem:quantum_raz}
Let $\rho$ and $\sigma$ be two CQ states with  $\rho^{XA}= \rho^{X_1 X_2 \ldots X_n A}$ and $\sigma= \sigma^{XA}= \sigma^{X_1}\otimes \sigma^{X_2}\otimes \ldots \otimes \sigma^{X_n} \otimes \sigma^A$ with $X=X_1 X_2 \ldots X_n$ classical in both states. Then
\begin{equation}\label{eqn:Raz_lemma1} \sum_{i=1}^n \I(X_i \, :\, A)_\rho \leq \D(\rho^{XA} \, \| \sigma^{XA})\;. \end{equation}
\end{lemma}

\begin{proof}
By the chain rule (\Cref{prop:divergence_chain_rule}) we have 
\begin{equation}\label{eqn:Raz_lemma2}
\D(\rho^{XA} \| \sigma^{XA})= \D(\rho^{X_1} \| \sigma^{X_1}) + \Ex_{x_1 \sim \rho^{X_1}} \D(\rho^{X_2}_{X_1=x_1}\| \sigma^{X_2}) + \ldots+ \Ex_{x \sim \rho^{X_1 \cdots X_n}} \D(\rho^{A}_{X=x} \| \sigma^{A}),
\end{equation}
where $x_1 \sim \rho^{X_1}$ means sampling $x_1$ according to the classical distribution $\rho^{X_1}$, and similarly for $x \sim \rho^{X_1 \cdots X_n}$. Consider the $i$-th term in~\eqref{eqn:Raz_lemma2}, for $i\in\{1,\ldots,n\}$. We have, via the chain rule followed by Quantum Gibbs's Inequality (\Cref{prop:divergence_gibbs_inequality}), 
\begin{equation}\label{eq:raz-1a}
\Ex_{x_{<i}\sim \rho^{X_1X_2\ldots X_{i-1}}} \D(\rho^{X_i}_{X_{< i}= x_{<i}} \| \sigma^{X_i}) = \D(\rho^{X_1 \cdots X_i} \|\rho^{X_1 \cdots X_{i-1}} \otimes \sigma^{X_i}) \geq  \I(X_1\ldots X_{i-1} : X_i)_\rho\;.
\end{equation}
Now consider the last term in~(\ref{eqn:Raz_lemma2}), again by Quantum Gibbs's Inequality, we have
\begin{align}
\Ex_{x\sim \rho^X} \D(\rho^{A}_{X=x} \|\sigma^{A}) &= \D(\rho^{XA} \| \rho^X \otimes \sigma^A) \geq \I(X:A)_\rho = \sum_{i=1}^n \I(X_i : A| X_1 X_2 \ldots X_{i-1})_\rho\;,\label{eq:raz-1b}
\end{align}
where the last equality is by definition of the conditional mutual information. 
Summing~\eqref{eq:raz-1a} for $i\in\{1,\ldots,n\}$ and~\eqref{eq:raz-1b} and using $\I(X_i:A X_1\ldots X_i)= \I(X_i:X_1\ldots X_{i-1})+ \I(X_i:A|X_1\ldots X_{i-1})$ gives, from~\eqref{eqn:Raz_lemma2},
\[ \D(\rho^{X A} \| \sigma^{XA})\geq \sum_{i=1}^n \I(X_i: A X_1 \ldots X_{i-1})_\rho \geq \sum_{i=1}^n \I(X_i:A)_\rho\;,
\]
where the last inequality follows by taking the difference and using strong subadditivity of conditional mutual information (\Cref{prop:ssa}). 
\end{proof}

Finally we define the \emph{fidelity} between two density matrices $\rho$ and $\sigma$ as 
\[\F(\rho,\sigma) = \| \sqrt{\rho} \sqrt{\sigma} \|_1\;.\]
The Fuchs-van de Graaf inequalities relate fidelity and trace norm as
\begin{equation}\label{eq:fuchs-graaf}
1 - \F(\rho,\sigma) \leq \frac{1}{2} \| \rho - \sigma \|_1 \leq \sqrt{1 - \F(\rho,\sigma)^2}\;.
\end{equation}
The following well-known result relates the fidelity between two density matrices with the inner product between their purifications. %and can be found in e.g.~\cite{wilde2013quantum}.

\begin{theorem}[Uhlmann's Theorem, Theorem 9.2.1 in~\cite{wilde2013quantum}]
\label{thm:uhlmann}
	Let $\ket{\psi}^{AB},\ket{\phi}^{AB}$ be bipartite states, and let $\rho^{A}$ and $\sigma^{A}$ denote their reduced density matrices on the system $A$, respectively. Then there exists a unitary map $V$ acting on $B$ such that
	\[
		\bra{\phi} (\Id \otimes V) \ket{\psi} = \F(\rho,\sigma)~.
	\]
%	Furthermore, we can assume without loss of generality that $V$ is such that the inner product $\bra{\phi} (\Id \otimes V) \ket{\psi}$ is a nonnegative real number.
\end{theorem}



\subsubsection{The proof}

Recall from Section~\ref{sec:states-operators} that the strategy $\strategy^n$ consists of the entangled state $\ket{\psi} \in \C^d_{E_A} \otimes \C^d_{E_B}$ and POVMs $\{A_\bx(\ba) \}$ acting on system $E_A$ and $\{B_\by(\bb) \}$ acting on system $E_B$. For all $\bomega$, $\ba_C$, and $\bb_C$, define
\begin{equation}
\label{eq:a-bomega-def}
	A_\bomega(\ba_C) = \Ex_{\bX | \bOmega = \bomega} \, A_\bx(\ba_C) \qquad \text{and} \qquad B_\bomega(\bb_C) = \Ex_{\bY | \bOmega = \bomega} \, B_\by(\bb_C)
\end{equation}
where $A_\bx(\ba_C),B_\by(\bb_C)$ are defined in~\cref{eq:states-operators-1}. 
We let $\rho$ denote the reduced density matrix of $\ket{\psi}$ on either system (this is well-defined because we assumed the form~\eqref{eq:psi-sym} for $\ket{\psi}$). 

%We first prove~\eqref{eq:vy_bound}, that is, the existence of the unitary $V_{\br_\mi, y}$. 
Recall the notation $\psi = \ket{\psi}\bra{\psi}$ and $X[\rho]=X\rho X^\dagger$. For a classical random variable, such as $\ba_C$, we write $\puretomixed{\ba_C}$ to denote the rank-$1$ density matrix $\ketbra{\ba_C}{\ba_C}$. Let $\ac$ denote the random variable pair $(\bA_C, \bB_C)$ so that $\bR_\mi = (\bOmega_\mi,\ac)$. Define the following states:
\begin{gather}
\Xi^{\bOmega \bY E_AE_B\ac} = \sum_{\bomega, \by, \ba_C, \bb_C} \P_{\bOmega \bY} (\bomega, \by) \, \puretomixed{\bomega, \by}   \otimes \left (\sqrt{A_{\bomega}(\ba_C)} \otimes \sqrt{B_{\by}(\bb_C)} \right ) \left [\psi \right ] \otimes \puretomixed{\ba_C, \bb_C}~, \label{eq:Xi-def} \\
\Lambda^{\bOmega \bX E_AE_B\ac} = \sum_{\bomega, \bx, \ba_C, \bb_C} \P_{\bOmega \bX} (\bomega, \bx) \, \puretomixed{\bomega, \bx}   \otimes \left (\sqrt{A_{\bx}(\ba_C)} \otimes \sqrt{B_{\bomega}(\bb_C)} \right ) \left [\psi \right ] \otimes \puretomixed{\ba_C, \bb_C}~. \label{eq:Lambda-def}
\end{gather}
The states are classical on the registers $\bOmega$, $\bX$, $\bY$, and $\ac$, and quantum on registers $E_A$ and $E_B$. %Observe that this state looks very similar -- but not quite -- to the one that occurs in an actual execution of the strategy $\strategy^n$. There are several important differences: one is that the measurements only produce answers for the coordinates indexed by $C$. Another difference is that the measurement operators $A_\bomega(\ba_C)$ are not part of the strategy but instead are derived from Alice's measurements $\{A_\bx\}$. Finally, note that there is no explicit register for Alice's question vector $\bX$ (except for the $\bX_C$ questions, which are included in the register $\bOmega$); instead these questions are implicitly averaged over within the $A_\omega$ measurement. 
The state $\Xi$ is defined such that by tracing out the entanglement registers $E_A$ and $E_B$, the resulting state $\Xi^{\bOmega \bY \bA_C \bB_C}$ is a classical state representing the probability distribution $\P_{\bOmega \bY \bA_C \bB_C}$. To see this, observe that
\begin{align*}
\Xi^{\bOmega \bY \bA_C \bB_C} &= \sum_{\bomega, \by, \ba_C, \bb_C} \P_{\bOmega \bY}(\bomega, \by) \, \puretomixed{\bomega, \by, \ba_C, \bb_C} \, \bra{\psi} A_\bomega(\ba_C) \otimes B_\by(\bb_C) \ket{\psi} \\
&= \sum_{\bomega, \by, \ba_C, \bb_C} \P_{\bOmega \bY}(\bomega, \by) \, \puretomixed{\bomega, \by, \ba_C, \bb_C} \, \bra{\psi} \Ex_{\bX | \bOmega = \bomega} \, A_\bx(\ba_C)  \otimes B_\by(\bb_C) \ket{\psi} \\
&= \sum_{\bomega, \bx, \by, \ba_C, \bb_C} \P_{\bOmega \bX \bY}(\bomega, \bx, \by) \, \puretomixed{\bomega, \by, \ba_C, \bb_C} \cdot \P_{\bA_C \bB_C | \bX = \bx, \bY = \bY}(\ba_C,\bb_C) \\
&= \sum_{\bomega,\by, \ba_C, \bb_C} \P_{\bOmega\bY \bA_C \bB_C}(\bomega, \by, \ba_C,\bb_C) \, \puretomixed{\bomega, \by, \ba_C, \bb_C}
\end{align*}
where in the third line we used the fact that $\P_{\bOmega \bX \bY} = \P_{\bOmega} \P_{\bX | \bOmega} \P_{\bY | \bOmega}$ and in the fourth line we used that $\P_{\bA_C \bB_C | \bX \bY} = \P_{\bA_C \bB_C | \bOmega \bX \bY}$. \tnote{These two facts should be included in a claim in Section~\ref{sec:dep-var}; see note there} \hnote{TODO} Similarly, the state $\Lambda^{\bOmega \bX \bA_C \bB_C}$ represents the probability distribution  $\P_{\bOmega \bX \bA_C \bB_C}$.

Since the event $W_C$ is determined by the random variables $(\bOmega,\bA_C,\bB_C)$ we can {condition} the states $\Xi, \Lambda$ on the event $W_C$ to obtain states
\begin{gather*}
	\xi^{\bOmega \bY E_A E_B \ac} = \frac{1}{\P(W_C)} \sum_{\substack{\bomega, \by, \ba_C, \bb_C: \\ (\bomega,\ba_C,\bb_C) \in W_C}} \P_{\bOmega \bY}(\bomega, \by) \, \puretomixed{\bomega, \by} \otimes \left (\sqrt{A_{\bomega}(\ba_C)} \otimes \sqrt{B_{\by}(\bb_C)} \right ) \left [\psi \right ] \otimes \puretomixed{\ba_C,\bb_C}~, \\
	\lambda^{\bOmega \bX E_A E_B \ac} = \frac{1}{\P(W_C)} \sum_{\substack{\bomega, \bx, \ba_C, \bb_C: \\ (\bomega,\ba_C,\bb_C) \in W_C}} \P_{\bOmega \bX}(\bomega, \bx) \, \puretomixed{\bomega, \bx} \otimes \left (\sqrt{A_{\bx}(\ba_C)} \otimes \sqrt{B_{\bomega}(\bb_C)} \right ) \left [\psi \right ] \otimes \puretomixed{\ba_C,\bb_C}\;.
\end{gather*}
We can further condition $\xi$ and $\Xi$ on specific settings of the classical variables $(\bOmega,\bY,\bA_C,\bB_C)$. For example, for any $\br = (\bomega,\ba_C,\bb_C)$ we write $\xi_\br$ and $\Xi_\br$ to denote $\xi$ and $\Xi$ conditioned on $\bOmega = \bomega, \bA_C = \ba_C,\bB_C = \bb_C$, respectively. As another example, for all $i \in [n] \setminus C$, $\br_\mi = (\bomega_\mi,\ba_C,\bb_C) \in W_C$, $x \in \X$ and $y \in \Y$, we write $\xi_{\br_\mi,x,y}$ and $\Xi_{\br_\mi,x,y}$ to denote $\xi$ and $\Xi$ respectively conditioned on $\bOmega = \bomega$ (where $\bomega = (\bomega_\mi,\bomega_i)$ with $\bomega_i = (A,x)$),  $\bA_C = \ba_C, \bB_C = \bb_C$, and $\bY_i = y$.

Similarly, let $\lambda_{\br_\mi,x,y}$ and $\Lambda_{\br_\mi,x,y}$ denote $\lambda$ and $\Lambda$ respectively conditioned on $\bOmega = \bomega$ (where $\bomega = (\bomega_\mi,\bomega_i)$ with $\bomega_i = (B,y)$), $\bA_C = \ba_C, \bB_C = \bb_C$, and $\bX_i = x$.
%
%\begin{gather}
%	\xi^{\bOmega \bY  E_AE_B \ac} = \Xi^{\bOmega \bY E_AE_B \ac|W_C} ,\label{eq:xi-def}\\
%	\xi^{E_A}_\sspyi = \xi^{E_A}_{\bR_\mi =\br_\mi,\bY_i=\by_i,\bomega_i=(A,\dummy)}.
%	%,\ac=z}\qquad\text{and}\qquad\xi^{E_B}_\sspp = \xi_{E_B|\bOmega_{-i}=\bomega_{-i},X_i=\perp,\bomega_i=(B,\perp),\ac=z}.
%	\label{eq:xisspp-def}
%\end{gather}
%The state $\Xi$ is defined so that tracing out the entanglement registers $E_A$ and $E_B$ the resulting state $\Xi^{\bOmega \bY \bA_C \bB_C}$ is a classical state that is equivalent to the probability distribution $\P_{\bOmega \bY \bA_C \bB_C}$, i.e.,
%\[
%\Xi^{\bOmega \bY \bA_C \bB_C} = \sum_{\bomega, \by, \ba_C, \bb_C} \P_{\bOmega \bY \bA_C \bB_C}(\bomega, \by, \ba_C, \bb_C) \, \puretomixed{\bomega, \by, \ba_C, \bb_C}~.
%\]
% In~\eqref{eq:xi-def} the conditioning on $W_C$ is well-defined since the event only involves classical random variables in $\bOmega$ and $\ac = (\bA_C,\bB_C)$. In~\eqref{eq:xisspp-def} only the reduced density on $E_A$ is considered, all other registers being traced out. %We also consider states such as $\xi^{Y^mE_A}_{\bomega,z} = \xi_{Y^mE_A|\bOmega=\bomega,\ac=z}$, where non-indexed registers are always considered to be traced out. 

The main step of the proof is given by the following two claims. 

\begin{claim}\label{claim:xi-change-x}
The following hold.
\begin{gather}
\Ex_I \,  \Ex_{\bR | W_C} \I \big (\bY_i ; E_A\big )_{\xi_\br} = \, O(\delta)~, \label{eq:xi-change-1}\\
\Ex_I \,  \Ex_{\bR | W_C} \I \big (\bX_i ; E_B\big )_{\lambda_\br} = \, O(\delta)~. \label{eq:xi-change-2}
%\Ex_I \, \Ex_{\bR \bY_i | W_C} \,\, \Big \| \xi^{E_A}_{\br,\by_i} -  \xi^{E_A}_{\br} \Big \|_1^2 \, =\, O(\delta)\;,
\end{gather}
%where the expectation over $Y$ is with respect to the game distribution $\mu_{Y}$. 
\end{claim}

\begin{proof}
We present the proof for~\eqref{eq:xi-change-1}; the proof for~\eqref{eq:xi-change-2} is similar. 
First we observe that by definition $\P(W_C) \, \xi\,\, \leq \,\Xi$. Using the definition of relative entropy and relative min-entropy this implies
\[ \D(\xi \| \Xi)\,\leq\, \D_\infty(\xi \| \Xi)\, \leq\, \log \frac{1}{\P(W_C)}\;.\]
  Using the chain rule for the relative entropy (\Cref{prop:divergence_chain_rule}) and the fact that tracing out registers can only decrease the relative entropy (\Cref{prop:divergence_data_processing}), 
\begin{equation}\label{eq:claim23-1}
	\Ex_{\bR | W_C} \D \big (\xi_{\br}^{\bY E_A} \,\, \| \,\, \Xi_{\br}^{\bY E_A} \big ) \leq \log \frac{1}{\P(W_C)}\;.
\end{equation}
For a symmetric entangled state of the form $\ket{\psi} = \sum \sqrt{\lambda_j} \ket{v_j}\ket{v_j} \in \C^d \otimes \C^d$ and for all linear operators $X,Y$ acting on $\C^d$, a simple calculation shows that
\[
	\Tr_{E_B} \Big( (X \otimes Y)\ketbra{\psi}{\psi}(X \otimes Y)^\dagger \Big) = X \, \sqrt{\rho} \, \overline{Y^\dagger Y} \, \sqrt{\rho} \, X^\dagger\;,
\]
where $\rho = \sum \lambda_j \ket{v_j}\bra{v_j}$ and the complex conjugate is taken with respect to the orthonormal basis $\{\ket{v_j}\}$.  
	Using this, for all $\bomega$
	\begin{align}
		\Xi_\bomega^{\bY E_A \ac} &= \sum_{\by, \ba_C, \bb_C} \P_{\bY | \bomega}(\by) \,\, \puretomixed{\by} \otimes \sqrt{A_\bomega(\ba_C)} \,\, \sqrt{\rho} \,\, \overline{B}_{\by}(\bb_C) \,\, \sqrt{\rho} \,\, \sqrt{A_\bomega(\ba_C)}  \otimes \puretomixed{\ba_C \bb_C}\notag \\
		&\leq  \sum_{\by, \ba_C, \bb_C} \P_{\bY | \bomega}(\by) \,\, \puretomixed{\by} \otimes \sqrt{A_\bomega(\ba_C)} \,\, \sqrt{\rho} \,\, \overline{B}_{\by}(\bb_C) \,\, \sqrt{\rho} \,\, \sqrt{A_\bomega(\ba_C)} \otimes \Id \notag\\
		&= \sum_{\by, \ba_C} \P_{\bY | \bomega}(\by) \,\, \puretomixed{\by} \otimes \sqrt{A_\bomega(\ba_C)} \,\,\rho\,\, \sqrt{A_\bomega(\ba_C)} \otimes \Id \notag \\
		&= \Xi_\bomega^{\bY} \otimes \Xi_\bomega^{E_A} \otimes \Id \;,\label{eq:claim23-2}
	\end{align}
where the second-to-last equality uses $\sum_{\bb_C} \overline{B}_{\by}(\bb_C) = \Id$ and in the last equality we used that the reduced density matrices
\[\Xi_\bomega^{\bY}\,=\,\sum_{\by} \P_{\bY | \bomega}(\by) \,\, \puretomixed{\by}\qquad\text{and}\qquad \Xi_\bomega^{E_A}\sum_{\ba_C} \sqrt{A_\bomega(\ba_C)} \,\,\rho\,\, \sqrt{A_\bomega(\ba_C)}\;.\]
Taking the partial trace on both sides of~\eqref{eq:claim23-2} we get
\begin{align}
\label{eq:local-unitaries-2}
\Xi_\bomega^{\bY E_A} \,\leq\, (|\A|\cdot |\B|)^{|C|} \,\, \Xi_{\bomega}^{\bY} \otimes \Xi_{\bomega}^{E_A}\;.
\end{align}
	From~\eqref{eq:local-unitaries-2} and the definition of $\D_\infty$ it follows that 
	\begin{equation}\label{eq:claim23-2b}
	\D_\infty \big (\Xi_{\bomega}^{\bY E_A} \Big \| \Xi_{\bomega}^{\bY} \otimes \Xi_{\bomega}^{E_A} \big ) \leq |C| \cdot \log |\A| |\B|\;.
	\end{equation}
Applying Lemma~\ref{lem:quantum_raz},
\begin{align*}
		\Ex_I \,  \Ex_{\bR | W_C} \I \Big (\bY_i ; E_A\Big )_{\xi_\br} &\leq 		\frac{1}{m} \Ex_{\bR | W_C} \D \Big (\xi_{\br}^{\bY E_A} \Big \| \Xi_{\br}^{\bY} \otimes \Xi_{\br}^{E_A} \Big )\notag\\
		&\leq \frac{1}{m} \Ex_{\bOmega | W_C} \D \Big (\xi_{\bomega}^{\bY E_A} \Big \| \Xi_{\bomega}^{\bY} \otimes \Xi_{\bomega}^{E_A} \Big )\notag \\
		&\leq 		\frac{1}{m} \Big(\Ex_{\bOmega | W_C} \D \Big (\xi_{\bomega}^{\bY E_A} \Big \| \Xi_{\bomega}^{\bY E_A} \Big) + \Ex_{\bOmega | W_C} \D_\infty \Big (\Xi_{\bomega}^{\bY E_A} \Big \| \Xi_{\bomega}^{\bY} \otimes \Xi_{\bomega}^{E_A} \Big) \, \Big)\notag\\
		&\leq \frac{1}{m}\Big(\log \frac{1}{\P(W)} +  |C| \cdot \log |\A| |\B|\Big) = \delta~.
\end{align*}
where the second inequality follows from \Cref{prop:divergence_chain_rule}, the third from \Cref{prop:relative_min_entropy_chain_rule2} and the last uses~\eqref{eq:claim23-1} and~\eqref{eq:claim23-2b}. This establishes~\eqref{eq:xi-change-1} of the Claim. The proof for~\eqref{eq:xi-change-2} is similar.
\end{proof}

The second claim relates the reduced densities on $E_A$ (resp. $E_B$) of the states $\xi$ (resp. $\lambda$) associated with different choices of $i,\br_\mi,x,y$. 

\begin{claim}\label{claim:xi-change-y}
The following hold:
\begin{gather}
\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \, \Ex_{XY} \,\, \Big \| \xi^{E_A}_\ssxy - \xi^{E_A}_\ssxp \Big \|_1^2\; = \; O\big(\sqrt{\delta}/\alpha^4 \big)~, \label{eq:E_A_1} \\
\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \, \Ex_{XY} \,\, \Big \| \lambda^{E_B}_\ssxy - \lambda^{E_B}_\sspy \Big \|_1^2\; = \; O\big(\sqrt{\delta}/\alpha^4 \big)~, \label{eq:E_A_2}
%\label{eq:E_A_2}
%\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \, \Ex_{\bX_i\bY_i} \,\, \Big \| \xi^{E_A}_\sspxiyi - \xi^{E_A}_\sspxip \Big \|_1^2 = O\big(\sqrt{\delta}/\alpha^2\big)
\end{gather}
where the expectation over $XY$ is with respect to the game distribution $\mu_{XY}$. 
\end{claim}

\begin{proof}
We present the proof for~\eqref{eq:E_A_1}; the proof of~\eqref{eq:E_A_2} is similar. Consider, for all $i$ and $\br$ that implies the event $W_C$, the state
\[
	\xi_{\br}^{\bY_i E_A} = \Ex_{\bY_i | \bR = \br, W_C} \, \puretomixed{\by_i}^{\bY_i} \otimes \xi_{\br,\by_i}^{E_A}
\]
and note that it is classical in the $\bY_i$ register.
Applying Pinsker's inequality (Lemma~\ref{prop:pinsker}) and then using \Cref{prop:mutual-information-classical} we get
\begin{align}
\Ex_I \, \Ex_{\bR \bY_i | W_C} \,\, \big \| \xi^{E_A}_{\br,\by_i} -  \xi^{E_A}_{\br} \Big \|_1^2 &\leq 2  \ln 2 \, \Ex_I \,\Ex_{\bR \bY_i | W_C} \,\,  \D \Big(  \xi^{E_A}_{\br,\by_i} \, \big \|\, \xi^{E_A}_{\br} \big )\notag\\
&= 2\ln 2\, \Ex_I \, \Ex_{\bR | W_C} \,\,  \I (\bY_i ; E_A)_{\xi_\br} \notag\\
& = O(\delta)~,\label{eq:claim24-1a}
\end{align} 
where the last line is by~\eqref{eq:xi-change-1}. Using \Cref{lem:trivial} to insert a copy of $\P_{\bX_i \bY_i}$ into both sides of the difference of Item 3 of \Cref{lem:classical_skew} yields
\[
\Ex_I \left \|  \P_{\bX_i \bY_i} \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bX_i = \dummy, \bY_i =\dummy,  W_C} - \P_{\bX_i \bY_i} \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bOmega_i W_C} \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\]
Inserting $\P_{\Omega_i | W_C}$ into both sides of the difference of Item 4 of \Cref{lem:classical_skew} yields
\[
\Ex_I \left \|  \P_{\bX_i \bY_i} \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bX_i = \dummy, \bY_i =\dummy,  W_C} - \P_{\bX_i \bY_i} \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bX_i \bY_i  W_C} \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\]
Using the triangle inequality with the previous two bounds yields
\begin{equation}
\label{eq:xi-change-y-1b}
\Ex_I \left \|  \P_{\bX_i \bY_i} \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bOmega_i  W_C} - \P_{\bX_i \bY_i}  \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bX_i \bY_i  W_C} \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\end{equation}
Marginalizing over $\bOmega_i$ in Item 1 of \Cref{lem:classical_skew} we get $\Ex_I \left \| \P_{\bX_i \bY_i} - \P_{\bX_i \bY_i | W_C} \right \| \leq \sqrt{\delta}$, so we can replace the second instance of $\P_{\bX_i \bY_i}$ in~\eqref{eq:xi-change-y-1b} with $\P_{\bX_i \bY_i | W_C}$ (incurring an error of $\sqrt{\delta}$) to get
%Items 1, 3, and 4 of \Cref{lem:classical_skew} imply that 
\[
\Ex_I \left \|  \P_{\bX_i \bY_i} \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bOmega_i  W_C} - \P_{\bX_i \bY_i | W_C}  \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bX_i \bY_i  W_C} \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\]
Marginalizing over $\bX_i \bY_i$ on both side yields
\begin{equation}\label{eq:claim24-1}
\Ex_I \left \| \P_{\bOmega_i | W_C} \P_{\bR_\mi | \bOmega_i  W_C} -  \P_{\bOmega_i | W_C} \P_{\bR_\mi | W_C} \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\end{equation}
We insert a copy of $\P_{\bY_i | \bOmega_i }$ in~\eqref{eq:claim24-1} to obtain via \Cref{lem:trivial}
\begin{equation}\label{eq:claim24-2}
\Ex_I \left \| \P_{\bOmega_i | W_C}\P_{\bR_\mi | \bOmega_i  W_C} \P_{\bY_i | \bOmega_i }  - \P_{\bOmega_i | W_C} \P_{\bR_\mi | W_C}  \P_{\bY_i | \bOmega_i } \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\end{equation}
Marginalizing over $\bX_i$ in Item 2 of~\Cref{lem:classical_skew} we obtain
\begin{equation}\label{eq:claim24-3}
\Ex_{I} \big\|\P_{\bR \bY_i | W_C} - \P_{\bR | W_C} \P_{\bY_i | \bOmega_i }\big\| \,\leq\,\sqrt{\delta}\;.
\end{equation}
Using $\bR = (\bR_\mi,\bOmega_i)$ it follows that $\P_{\bR | W_C}=\P_{\bOmega_i | W_C} \P_{\bR_\mi | \bOmega_i  W_C}$. Thus~\eqref{eq:claim24-3} implies that we can replace the first instance of $\P_{\bOmega_i | W_C} \P_{\bR_\mi | \bOmega_i  W_C}$ in~\eqref{eq:claim24-2} with $\P_{\bR \bY_i | W_C}$ (incurring an error of $\sqrt{\delta}$) to get
\begin{equation}\label{eq:claim24-4}
\Ex_I \left \| \P_{\bR \bY_i | W_C} -\P_{\bOmega_i | W_C} \P_{\bR_\mi | W_C}  \P_{\bY_i | \bOmega_i }  \right \| \leq O(\sqrt{\delta}/\alpha^2)~.
\end{equation}
Combining~\eqref{eq:claim24-4} with~\eqref{eq:claim24-1a} gives
\begin{equation}\label{eq:claim24-4c}
	\Ex_I \, \Ex_{\bOmega_i | W_C} \, \Ex_{\bR_\mi | W_C} \, \Ex_{\bY_i | \bOmega_i} \,\, \big \| \xi^{E_A}_{\br,\by_i} -  \xi^{E_A}_{\br} \big \|_1^2 \leq O(\sqrt{\delta}/\alpha^2)\;.
\end{equation}
Using the fact that $\Ex_I \| \P_{\bOmega_i | W_C} - \P_{\bOmega_i} \| \leq \sqrt{\delta}$, which follows from Item 1 of \Cref{lem:classical_skew}, recalling that $\bOmega_i = (\bM_i,\bD_i)$, conditioning on $\bD_i = A$ (which occurs with probability $1/2$) from~\eqref{eq:claim24-4c} we get
\begin{equation}\label{eq:claim24-4d}
	\Ex_I \, \Ex_{\bM_i | \bD_i =A} \, \Ex_{\bR_\mi | W_C} \, \Ex_{\bY_i | \bOmega_i=(A,\bM_i)} \,\, \Big \| \xi^{E_A}_{\br,\by_i} -  \xi^{E_A}_{\br} \Big \|_1^2 \leq O(\sqrt{\delta}/\alpha^2).
\end{equation}
Next our goal is to exchange the expectations over $\bM_i | \bD_i = A$ and $\bY_i | \bOmega_i = (A,\bM_i)$ with an expectation over $XY$. From the definition of the variables $(D,M,X,Y)$ (see \Cref{sec:p_setup}) we see that
\[\forall x\in \X\;,\qquad \P_X(x) \,\leq\, \frac{1 - \eta}{\alpha - \eta} \, \P_{M|D=A}(x)\;.\] 
Therefore for all $x \in \X$
\begin{equation}
\label{eq:xi-change-y-1}
	\P_{XY}(x,y) = \P_{X}(x) \cdot \P_{Y|X=x}(y) \leq \frac{2}{\alpha} \,\P_{\bM_i | \bD_i = A}(x) \cdot \P_{\bY_i | \bOmega_i = (A,x)}(y)
\end{equation}
where we used $\P_{\bY_i | \bOmega_i = (A,x)}(y) = \P_{Y|X=x}(y)$ and $\eta = \alpha/2$. Furthermore observe that when $\br = (\br_\mi,\bomega_i)$ with $\bomega_i = (A,x)$ then by definition
\begin{equation}
\label{eq:xi-change-y-2}
	\xi_{\br,y} = \xi_{\br_\mi,x,y} \qquad \text{and} \qquad \xi_{\br} = \xi_{\br_\mi,x}~.
\end{equation}
%We also have that $\P_{\bY_i | \bOmega_i = (A,x)}(y) = \P_{Y | X}(y|x)$. 
Thus
\begin{align}
	\Ex_I \, \Ex_{XY} \, \Ex_{\bR_\mi | W_C} \, \,\, \Big \| \xi^{E_A}_{\br_\mi,x,y} -  \xi^{E_A}_{\br_\mi,x} \Big \|_1^2 &\leq \frac{2}{\alpha} \, \Ex_I \, \Ex_{\bM_i \bY_i | \bD_i =A} \, \Ex_{\bR_\mi | W_C} \,\, \Big \| \xi^{E_A}_{\br,\by_i} -  \xi^{E_A}_{\br} \Big \|_1^2 \notag\\
	&\leq O(\sqrt{\delta}/\alpha^3)\;,\label{eq:xi-change-y-3}
\end{align}
where the first inequality is by~\eqref{eq:xi-change-y-1} and uses~\eqref{eq:xi-change-y-2} and the second is by~\eqref{eq:claim24-4d}. 
Conditioning on $Y = \dummy$, which occurs with probability at least $\alpha$ under the distribution $\mu_Y$, yields
\begin{equation}
\label{eq:xi-change-y-4}
	\Ex_I \, \Ex_{X} \, \Ex_{\bR_\mi | W_C} \, \,\, \Big \| \xi^{E_A}_{\br_\mi,x,\dummy} -  \xi^{E_A}_{\br_\mi,x} \Big \|_1^2 \leq O(\sqrt{\delta}/\alpha^4)~.
\end{equation}
Using the triangle inequality and $(a+b)^2 \leq 2(a^2+b^2)$ we get 
\begin{align*}
\Ex_I \, \Ex_{XY} \, \Ex_{\bR_\mi | W_C} \, \,\, \Big \| \xi^{E_A}_{\br_\mi,x,\dummy} -  \xi^{E_A}_{\br_\mi,x,y} \Big \|_1^2 &\leq \Ex_I \, \Ex_{XY} \, \Ex_{\bR_\mi | W_C} \, \,\, 2 \, \Big \| \xi^{E_A}_{\br_\mi,x,\dummy} -  \xi^{E_A}_{\br_\mi,x} \Big \|_1^2 + 2 \,\, \Big \| \xi^{E_A}_{\br_\mi,x} -  \xi^{E_A}_{\br_\mi,x,y} \Big \|_1^2 \\
& \leq  O(\sqrt{\delta}/\alpha^4)\;,
\end{align*}`
where the second inequality uses~\eqref{eq:xi-change-y-3} and~\eqref{eq:xi-change-y-4}. This concludes the proof of~\eqref{eq:E_A_1} in \Cref{claim:xi-change-y}. The proof of~\eqref{eq:E_A_2} is similar.
%
%By the triangle inequality we have
%\[
%\Ex_I \, \Ex_{\bR | \bOmega_i=(A,\dummy) ,W_C} \,\, \Ex_{\bY_i | \br} \Big \| \xi^{E_A}_{\sspyi} -  \xi^{E_A}_{\sspp} \Big \|_1^2 \leq O(\delta/\alpha)~.
%\]
%We first note that the inner expectation $\Ex_{\bY_i | \br}$ can be switched to the expectation $\Ex_{\bY_i}$~; this is first because conditioned on $\bOmega_i = (A,\dummy)$, the random variable $\bY_i$ is independent of $\bR$ due to the definition of anchored game. After making this switch, notice that nothing within the outer expectation depends on $\bR_i$, and we notice that the outer expectation $\Ex_{\bR | \bOmega_i=(A,\dummy) ,W_C}$ can be switched to the expectation $\Ex_{\bR_\mi | \bX_i = \dummy ,W_C}$. In other words, the marginal distribution of $\bR_\mi$ conditioned on $\bOmega_i = (A,\dummy)$ and $W_C$ is the same as being conditioned on $\bX_i = \dummy$ and $W_C$ instead. Thus
%\[
%\Ex_I \, \Ex_{\bR_\mi | \bX_i = \dummy ,W_C} \,\, \Ex_{\bY_i} \Big \| \xi^{E_A}_{\sspyi} -  \xi^{E_A}_{\sspp} \Big \|_1^2 \leq O(\delta/\alpha)~.
%\]
%Item 3 of Lemma~\ref{lem:classical_skew} implies that the expectation $\Ex_{\bR_\mi | \bX_i = \dummy ,W_C}$ can be replaced by $\Ex_{\bR_\mi | W_C}$ while incurring an error of at most $O(\sqrt{\delta}/\alpha^2)$, which then concludes the proof of \Cref{claim:xi-change-y}.
%$$\P_I \cdot \P_{\bR \bY_i|\bOmega_i=(A,\dummy), W_C} \approx_{O(\sqrt{\delta}/\alpha^2)} \P_I \cdot \P_{\bR_\mi|W_C} \cdot \P_{\bY_i}.$$
\end{proof}

We are now ready to give the proof of Lemma~\ref{lem:unitary_bounds}.

\begin{proof}[Proof of Lemma~\ref{lem:unitary_bounds}]
We start by showing the existence of unitaries $V_{\br_\mi,y}$ that satisfy~\eqref{eq:vy_bound}. 
%First, note that when $\bomega_i = (A,x)$ for some $x \in \X$, then $A_\bomega(\ba_C)$ is equal to $A_{\bomega_\mi,\dummyx}(\ba_C)$ (defined in Eq.~\eqref{eq:states-operators-3}). This is because given $\bomega_i = (A,x)$, the random variable $\bX_i$ takes on the value $\dummy$ with probability $\eta$ and is $x$ with probability $1 - \eta$. In particular when $x = \dummy$, then $A_\bomega(\ba_C) = A_{\bomega_\mi,\dummy}(\ba_C)$. 
%The proof of~\eqref{eq:vy_bound} essentially now follows from Claim~\ref{claim:xi-change-y} and Uhlmann's theorem. We give the details. 
Let $\br_\mi = (\bomega_\mi,\ba_C,\bb_C)$ be such that it implies the event $W_C$, and let $x \in \X$, $y \in \Y$. 

\begin{claim}
\label{clm:xi-purification}
The state $\ket{\wt{\Phi}_\sspxy}^{E_A E_B}$ defined in~\eqref{eq:def-psitt} is a purification of the state $\xi_\ssxy^{E_A}$. Furthermore, the state $\ket{\wt{\Phi}_\sspy}^{E_A E_B}$ is a purification of the state $\xi_\sspy^{E_A}$. 
\end{claim}

\begin{proof}
Let $\bomega = (\bomega_\mi,\bomega_i)$ where $\bomega_i = (A,x)$. We can write $\xi_\ssxy$ explicitly as
\begin{align*}
	\xi_\ssxy = \frac{1}{\P_{\bOmega \bY_i \bA_C \bB_C}(\bomega,y,\ba_C,\bb_C)} \sum_{\by | \by_i = y} \P_{\bOmega \bY}(\bomega, \by) \, \puretomixed{\bomega, \by} \otimes \left (\sqrt{A_{\bomega}(\ba_C)} \otimes \sqrt{B_{\by}(\bb_C)} \right ) \left [\psi \right ] \otimes \puretomixed{\ba_C,\bb_C}\;,
\end{align*}
where $\br = (\br_\mi,\bomega_i)$. To see that the normalization is correct, the trace of the right-hand side (without the normalization) is
\begin{align*}
	\sum_{\by | \by_i = y} \P_{\bOmega \bY}(\bomega, \by) \, \bra{\psi} A_\bomega(\ba_C) \otimes B_\by(\bb_C) \ket{\psi} &= \sum_{\bx,\by | \by_i = y} \P_{\bOmega \bX \bY \bA_C \bB_C}(\bomega, \bx, \by, \ba_C, \bb_C) \\
	&= \P_{\bOmega \bY_i \bA_C \bB_C}(\bomega,y,\ba_C,\bb_C)~.
\end{align*}
Taking the partial trace of $\xi_\ssxy$ on the register $E_A$ we get
\begin{align}
\xi_\ssxy^{E_A} = &\frac{1}{\P_{\bOmega \bY_i \bA_C \bB_C}(\bomega,y,\ba_C,\bb_C)} \sum_{\by | \by_i = y} \P_{\bOmega \bY}(\bomega, \by) \, \sqrt{A_{\bomega}(\ba_C)} \,\, \sqrt{\rho} \,\, \overline{B}_\by(\bb_C) \,\, \sqrt{\rho} \,\, \sqrt{A_\bomega(\ba_C)}  \notag \\
&= \frac{\P_{\bOmega \bY_i}(\bomega, y)}{\P_{\bOmega \bY_i \bA_C \bB_C}(\bomega,y,\ba_C,\bb_C)} \, \sqrt{A_{\bomega}(\ba_C)} \,\, \sqrt{\rho} \,\, \Big ( \sum_{\by_\mi } \P_{\bY_\mi | \bomega_\mi}(\by_\mi) \overline{B}_\by(\bb_C) \Big) \,\, \sqrt{\rho} \,\, \sqrt{A_\bomega(\ba_C)} \notag \\
&= \frac{1}{\P_{\bA_C \bB_C | \bOmega = \bomega, \by_i = y}(\ba_C,\bb_C)} \, \sqrt{A_{\bomega}(\ba_C)} \,\, \sqrt{\rho} \,\,  \overline{B}_{\bomega_\mi, y} (\bb_C) \,\, \sqrt{\rho} \,\, \sqrt{A_\bomega(\ba_C)} \notag \\
&= \gamma_\sspxy^{-2} \, \sqrt{A_{\bomega}(\ba_C)} \,\, \sqrt{\rho} \,\,  \overline{B}_{\bomega_\mi, y} (\bb_C) \,\, \sqrt{\rho} \,\, \sqrt{A_\bomega(\ba_C)}
\label{eq:xi-purification-1}
\end{align}
where the operator $B_{\bomega_\mi, y} (\bb_C)$ is defined in \Cref{sec:states-operators} and the last line follows from~\eqref{eq:gamma-def-3} in \Cref{prop:gamma}. Notice that since $\bomega_i = (A,x)$, the operator $A_\bomega(\ba_C)$ is equal to $A_{\bomega_\mi,\dummyx}(\ba_C)$, defined in~\eqref{eq:states-operators-3}. This implies that the state in~\eqref{eq:xi-purification-1} is also the reduced density matrix of $\ket{\wt{\Phi}_\sspxy}^{E_A E_B}$ (defined in~\eqref{eq:def-psitt}) on register $E_A$. 

The ``Furthermore'' part of the Claim follows from the fact that when $x = \dummy$, the operator $A_\bomega(\ba_C)$ for $\omega_i = (A,x)$ is equal to $A_{\bomega_\mi,\dummy}(\ba_C)$, defined in~\eqref{eq:states-operators-2}. Thus $\ket{\wt{\Phi}_\sspxy}^{E_A E_B} = \ket{\wt{\Phi}_\sspy}^{E_A E_B}$, which concludes the proof.
\end{proof}

\Cref{clm:xi-purification} implies that the states $\bigket{\wt{\Phi}_\sspy}$ and $\bigket{\wt{\Phi}_\sspp}$ purify $\xi^{E_A}_\sspy$ and $\xi^{E_A}_\sspp$ respectively, so by Uhlmann's Theorem (\Cref{thm:uhlmann}), there exists a unitary $V_{\br_\mi, y}$ acting on $E_B$ such that 
\begin{align}
	\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \Ex_{Y} \,\,  \bigbra{\wt{\Phi}_\sspy } V_{\br_\mi, y}  \bigket{\wt{\Phi}_\sspp}  
	&= \Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \Ex_{Y}  \,\, \F \left (\xi_\sspy^{E_A}, \xi_\sspp^{E_A} \right)  \notag \\
	& \geq 1 - \frac{1}{2} \Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \Ex_{Y}  \,\,  \Big \| \xi^{E_A}_\sspy - \xi^{E_A}_\sspp \Big \|_1 \notag \\
	&\geq 1 - \frac{1}{2} \sqrt {\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \Ex_{Y}  \,\,  \Big \| \xi^{E_A}_\sspy - \xi^{E_A}_\sspp \Big \|_1^2} \notag \\
	& \geq 1 - O(\delta^{1/4}/\alpha^{5/2})\;,
\end{align}
where the second line follows from the Fuchs-van de Graaf inequality (Eq.~\eqref{eq:fuchs-graaf}) the third uses Jensen's inequality, and the fourth line uses Claim~\ref{claim:xi-change-y} by conditioning on $X = \dummy$, which occurs with probability $\alpha$. Translating from inner products to Euclidean distance and applying Jensen's inequality we get
\begin{align*}
	\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \Ex_{Y} \,\,   \left \| \bigket{\wt{\Phi}_\sspy } - V_{\br_\mi, y}  \bigket{\wt{\Phi}_\sspp}  \right \|& \leq \sqrt{\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \Ex_{Y} \,\,   \left \| \bigket{\wt{\Phi}_\sspy } - V_{\br_\mi, y}  \bigket{\wt{\Phi}_\sspp}  \right \|^2}\\ &\leq O(\delta^{1/8}/\alpha^{5/4})~.
\end{align*}
Applying Markov's inequality over the index $i$ establishes~\eqref{eq:vy_bound}.

The argument for~\eqref{eq:vxy_bound} proceeds similarly; we start by using \Cref{clm:xi-purification} again, which states that the states $\bigket{\wt{\Phi}_\sspxy}$ and $\bigket{\wt{\Phi}_\sspxp}$ purify the reduced density matrices $\xi^{E_A}_\sspxy$ and $\xi^{E_A}_\sspxp$ respectively. Using Uhlmann's Theorem and Claim~\ref{claim:xi-change-y} in a similar way to how we derived~\eqref{eq:vy_bound}, we deduce the existence of unitaries $V_{\ssxy}$ satisfying~\eqref{eq:vxy_bound}.

To prove~\eqref{eq:ux_bound} we use the following Claim, whose proof is analogous to that of \Cref{clm:xi-purification}.
\begin{claim}
\label{clm:lambda-purification}
The state $\bigket{\wt{\Phi}_\ssxp}^{E_A E_B}$, defined in~\eqref{eq:def-psitt}, is a purification of the state $\lambda_\ssxp^{E_B}$. 
\end{claim}

Using this, we can use Uhlmann's Theorem again to deduce the existence of unitaries $U_{\br_\mi,x}$ that satisfies~\eqref{eq:ux_bound}. 

%
%The main difference is that  we need to analyze the state $\Lambda$ defined as
%\[
%\Lambda^{\bOmega \bX E_AE_B\ac} = \sum_{\bomega, \bx, \ba_C, \bb_C} \P_{\bOmega \bX} (\bomega, \bx) \, \puretomixed{\bomega, \bx}   \otimes \left (\sqrt{A_{\bx}(\ba_C)} \otimes \sqrt{B_{\bomega}(\bb_C)} \right ) \left [\psi \right ] \otimes \puretomixed{\ba_C, \bb_C}~,
%\]
%as well as the state $\lambda$ which is $\Lambda$ conditioned on the event $W_C$. The states $\lambda$ and $\Lambda$ are analogous to $\xi$ and $\Xi$ except that they keep track of Alice's question vector $\bX$ instead of $\bY$. The following claim can be shown in the same way as in \Cref{claim:xi-change-y}. \tnote{I think this is a bit cavalier. Maybe we could at least define the $\Lambda$ and state the claim right after \Cref{claim:xi-change-y}, rather than adding it here as an afterthought.}
%
%\begin{claim}\label{claim:yi-change-x}
%The following holds:
%\begin{align}
%\label{eq:E_B_1}
%\Ex_I \, \Ex_{\bR_\mi |  W_C} \, \, \, \Ex_{XY} \,\, \Big \| \lambda^{E_B}_\ssxy - \lambda^{E_B}_\sspy \Big \|_1^2 = O\big(\sqrt{\delta}/\alpha^3 \Big)\;,
%\end{align}
%where the expectation over $XY$ is with respect to the game distribution $\mu_{XY}$. 
%\end{claim}


\end{proof}




%
%
%
%%Expanding out the squared Euclidean norm and making sure that $V_{\br_\mi,y_i}$ is chosen so as to ensure that the inner product $\bra{\wt{\Phi}_\sspyi} V_{\br_\mi,y_i} \ket{\wt{\Phi}_\sspp}$ is positive real,~\eqref{eq:ux_bound_1} proves~\eqref{eq:vy_bound}.
%
%A nearly identical argument yields~\eqref{eq:ux_bound}. It remains to show~\eqref{eq:vxy_bound}. Define
%$$
%	\xi^{E_A}_{\sspxiyi} = \frac{1}{2} \xi^{E_A}_{\sspyi} + \frac{1}{2} \xi^{E_A}_{\ssxiyi} \qquad \text{and} \qquad \xi^{E_A}_{\sspxip} = \frac{1}{2} \xi^{E_A}_{\sspp} + \frac{1}{2} \xi^{E_A}_{\ssxip}
%$$
%For notational clarity, we will suppress mention of $\bomega_\mi$ and $z$; it will be implicitly carried around. 
%
%The density matrices $\xi^{E_A}_{\pxiyi}$ and $\xi^{E_A}_{\pxip}$ are purified by  $\ket{\wt{\Phi}_{\pxiyi}}$ and $\ket{\wt{\Phi}_{\pxip}}$ respectively. We will show that these two density matrices are close to together, on average, and hence by Uhlmann's Theorem implies that there exists a unitary $V_{x_i,y_i}$ acting on $E_B$ that moves $\ket{\wt{\Phi}_{\pxiyi}}$ close to $\ket{\wt{\Phi}_{\pxip}}$. Consider:
%\begin{align*}
%	\Ex_I \,\, \Ex_{\bOmega_\mi Z | W} \,\, \Ex_{X_i Y_i} \left \| \xi^{E_A}_{\pxiyi} - \xi^{E_A}_{\pp} \right \|_1 &= \Ex_I \,\, \Ex_{\bOmega_\mi Z | W} \,\, \Ex_{X_i Y_i} \left \| \frac{1}{2} \xi^{E_A}_{\pyi} + \frac{1}{2} \xi^{E_A}_{\xiyi} - \xi^{E_A}_{\pp} \right \|_1 \\
%	&\leq \Ex_I \,\, \Ex_{\bOmega_\mi Z | W} \,\, \Ex_{X_i Y_i} \left [ \frac{1}{2} \left \| \xi^{E_A}_{\pyi} - \xi^{E_A}_{\pp} \right \|_1 + \frac{1}{2} \left \| \xi^{E_A}_{\xiyi}  - \xi^{E_A}_{\pp} \right \|_1  \right].
%\end{align*}
%We obtained a bound on the first term in the calculations above. It remains to bound the second term. Again Lemma~\ref{lem:classical_skew} implies
%$$\P_I \cdot \P_{\bOmega \ac Y_i|D_i=A, W} \cong_{O(\sqrt{\delta}/\alpha^2)} \P_I \cdot \P_{\bR_\mi|W_C} \cdot \P_{X_i Y_i}$$
%where ``$\cong$'' indicates approximate equality, up to relabeling the random variable $M_i$ with $X_i$, whose marginals are identical conditioned on $D_i = A$. Thus using the same approach as earlier in the proof, we can obtain the bound
%$$
%\Ex_I \,\, \Ex_{\bOmega_\mi Z | W} \,\, \Ex_{X_i Y_i} \left \| \xi^{E_A}_{\xiyi}  - \xi^{E_A}_{\pp} \right \|_1 \leq O(\sqrt{\delta}/\alpha^4).
%$$
%Thus there exists the desired unitary $V_{x_i,y_i}$ such that
%\begin{align}
%	\frac{1}{m} \sum_i \Ex_{\bR_\mi |  W} \,\, \Ex_{X_i Y_i}  \, \left \| \bigket{\wt{\Phi}_\pxip } - V_{x_i,y_i}  \bigket{\wt{\Phi}_\pxiyi}  \right \|^2 \leq  O(\delta^{1/4}/\alpha^{4})
%\end{align}
%proving~\eqref{eq:vxy_bound}.
%\end{proof}


